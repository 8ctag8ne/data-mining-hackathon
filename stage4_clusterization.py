import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import pickle

# --- 1. –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è ---
df = pd.read_csv("user_features.csv")

print("=" * 70)
print("–û–ë–†–û–ë–ö–ê NULL –ó–ù–ê–ß–ï–ù–¨ –£ time_to_first_message_sec")
print("=" * 70)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ NULL –∑–Ω–∞—á–µ–Ω—å
null_count = df["time_to_first_message_sec"].isna().sum()
print(f"\n–ö—ñ–ª—å–∫—ñ—Å—Ç—å NULL –∑–Ω–∞—á–µ–Ω—å: {null_count}")

if null_count > 0:
    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è —Å–µ—Ä–µ–¥ –Ω–µ–Ω—É–ª—å–æ–≤–∏—Ö –∑–∞–ø–∏—Å—ñ–≤
    max_ttfm = df["time_to_first_message_sec"].max()
    print(f"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è time_to_first_message_sec: {max_ttfm:.2f}")

    # –ó–∞–º—ñ–Ω—è—î–º–æ NULL –Ω–∞ 2*MAX
    replacement_value = 2 * max_ttfm
    print(f"NULL –∑–Ω–∞—á–µ–Ω–Ω—è –±—É–¥—É—Ç—å –∑–∞–º—ñ–Ω–µ–Ω—ñ –Ω–∞: {replacement_value:.2f}")

    df["time_to_first_message_sec"].fillna(replacement_value, inplace=True)
    print(f"‚úì –ó–∞–º—ñ–Ω–µ–Ω–æ {null_count} NULL –∑–Ω–∞—á–µ–Ω—å")
else:
    print("‚úì NULL –∑–Ω–∞—á–µ–Ω–Ω—è –≤—ñ–¥—Å—É—Ç–Ω—ñ")

# –ó–±–µ—Ä—ñ–≥–∞—î–º–æ max_ttfm –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –∑ –Ω–æ–≤–∏–º–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º–∏
max_ttfm_value = df["time_to_first_message_sec"].max()

# --- 2. –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ —Ñ—ñ—á—ñ ---
features = [
    "answer_errors",
    "avg_event_interval_sec",
    "chat_opens",
    "chat_views",
    "dislikes",
    "error_rate",
    "likes",
    "messages_received",
    "messages_sent",
    "total_events",
    "time_to_first_message_sec",
    "like_dislike_ratio"
]

X = df[features].copy()

print(f"\n–§—ñ—á—ñ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó: {len(features)}")
print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤: {len(X)}")

# --- 3. –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è ---
print("\n" + "=" * 70)
print("–ú–ê–°–®–¢–ê–ë–£–í–ê–ù–ù–Ø –î–ê–ù–ò–•")
print("=" * 70)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
print("‚úì –î–∞–Ω—ñ –≤—ñ–¥–º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ")

# --- 4. Elbow-–≥—Ä–∞—Ñ—ñ–∫ ---
print("\n" + "=" * 70)
print("ELBOW METHOD")
print("=" * 70)

inertias = []
K_range = range(2, 10)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42)
    km.fit(X_scaled)
    inertias.append(km.inertia_)
    print(f"k={k}: inertia={km.inertia_:.2f}")

plt.figure(figsize=(10, 6))
plt.plot(K_range, inertias, marker="o", linewidth=2, markersize=8)
plt.title("Elbow Method", fontsize=14, fontweight='bold')
plt.xlabel("Number of Clusters (k)", fontsize=12)
plt.ylabel("Inertia", fontsize=12)
plt.grid(True, alpha=0.3)
plt.savefig("elbow_plot.png", dpi=200, bbox_inches='tight')
plt.show()

# --- 5. Silhouette Score ---
print("\n" + "=" * 70)
print("SILHOUETTE SCORE")
print("=" * 70)

sil_scores = {}

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42)
    labels = km.fit_predict(X_scaled)
    score = silhouette_score(X_scaled, labels)
    sil_scores[k] = score
    print(f"k={k}: silhouette={score:.4f}")

best_k = max(sil_scores, key=sil_scores.get)
print(f"\n‚úì Best k by silhouette score: {best_k} (score={sil_scores[best_k]:.4f})")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è Silhouette Scores
plt.figure(figsize=(10, 6))
plt.plot(list(sil_scores.keys()), list(sil_scores.values()),
         marker="o", linewidth=2, markersize=8, color='green')
plt.title("Silhouette Score by Number of Clusters", fontsize=14, fontweight='bold')
plt.xlabel("Number of Clusters (k)", fontsize=12)
plt.ylabel("Silhouette Score", fontsize=12)
plt.grid(True, alpha=0.3)
plt.axvline(x=best_k, color='red', linestyle='--', label=f'Best k={best_k}')
plt.legend()
plt.savefig("silhouette_plot.png", dpi=200, bbox_inches='tight')
plt.show()

# --- 6. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è ---
print("\n" + "=" * 70)
print(f"–ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–Ü–Ø (k={best_k})")
print("=" * 70)

model = KMeans(n_clusters=best_k, random_state=42)
df["cluster"] = model.fit_predict(X_scaled)

cluster_counts = df["cluster"].value_counts().sort_index()
print("\n–†–æ–∑–ø–æ–¥—ñ–ª –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö:")
for cluster_id, count in cluster_counts.items():
    print(f"  Cluster {cluster_id}: {count} –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ ({count / len(df) * 100:.1f}%)")

# --- 7. PCA –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó ---
print("\n" + "=" * 70)
print("PCA –í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø")
print("=" * 70)

pca = PCA(n_components=2)
coords = pca.fit_transform(X_scaled)
df["pca1"] = coords[:, 0]
df["pca2"] = coords[:, 1]

explained_var = pca.explained_variance_ratio_
print(f"PCA –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ –ø–æ—è—Å–Ω—é—é—Ç—å {sum(explained_var) * 100:.1f}% –≤–∞—Ä—ñ–∞—Ü—ñ—ó")
print(f"  PC1: {explained_var[0] * 100:.1f}%")
print(f"  PC2: {explained_var[1] * 100:.1f}%")

# –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ —É PCA –ø—Ä–æ—Å—Ç–æ—Ä—ñ
plt.figure(figsize=(12, 8))
colors = plt.cm.tab10(np.linspace(0, 1, best_k))

for i in range(best_k):
    cluster_data = df[df["cluster"] == i]
    plt.scatter(cluster_data["pca1"], cluster_data["pca2"],
                c=[colors[i]], label=f"Cluster {i}", alpha=0.6, s=50)

plt.xlabel(f"PC1 ({explained_var[0] * 100:.1f}% variance)", fontsize=12)
plt.ylabel(f"PC2 ({explained_var[1] * 100:.1f}% variance)", fontsize=12)
plt.title("User Clusters (PCA Visualization)", fontsize=14, fontweight='bold')
plt.legend()
plt.grid(True, alpha=0.3)
plt.savefig("clusters_pca.png", dpi=200, bbox_inches='tight')
plt.show()

# --- 8. –û–ø–∏—Å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ ---
print("\n" + "=" * 70)
print("–ü–†–û–§–Ü–õ–Ü –ö–õ–ê–°–¢–ï–†–Ü–í")
print("=" * 70)

cluster_profile = df.groupby("cluster")[features].mean()
cluster_size = df.groupby("cluster").size().rename("count")
cluster_churn = df.groupby("cluster")["is_churned"].mean().rename("churn_rate")

# –û–±—á–∏—Å–ª—é—î–º–æ avg_ratio (likes / dislikes) –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
cluster_avg_ratio = df.groupby("cluster").apply(
    lambda g: g["likes"].sum() / g["dislikes"].sum() if g["dislikes"].sum() > 0 else (
        float('inf') if g["likes"].sum() > 0 else 0
    )
).rename("avg_ratio")

# –ó–∞–º—ñ–Ω—é—î–º–æ inf –Ω–∞ –º–∞–∫—Å–∏–º–∞–ª—å–Ω–µ –∑–Ω–∞—á–µ–Ω–Ω—è + 1
max_finite_avg_ratio = cluster_avg_ratio[cluster_avg_ratio != float('inf')].max()
if pd.notna(max_finite_avg_ratio) and max_finite_avg_ratio > 0:
    replacement_avg_ratio = max_finite_avg_ratio + 1
else:
    replacement_avg_ratio = 100
cluster_avg_ratio = cluster_avg_ratio.replace(float('inf'), replacement_avg_ratio)

cluster_info = pd.concat([cluster_size, cluster_churn, cluster_avg_ratio, cluster_profile], axis=1)
cluster_info.to_csv("cluster_profiles.csv")

print("\nCluster profiles:")
print(cluster_info.round(2))

# --- 9. –î–æ–¥–∞—Ç–∫–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ ---
print("\n" + "=" * 70)
print("–î–û–î–ê–¢–ö–û–í–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
print("=" * 70)

# Churn rate –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
print("\nChurn rate –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö:")
for cluster_id in sorted(df["cluster"].unique()):
    cluster_data = df[df["cluster"] == cluster_id]
    churn_rate = cluster_data["is_churned"].mean()
    churn_count = cluster_data["is_churned"].sum()
    print(f"  Cluster {cluster_id}: {churn_rate * 100:.1f}% ({churn_count}/{len(cluster_data)})")

# Advanced users –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
print("\nAdvanced users –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö:")
for cluster_id in sorted(df["cluster"].unique()):
    cluster_data = df[df["cluster"] == cluster_id]
    advanced_rate = cluster_data["is_advanced"].mean()
    advanced_count = cluster_data["is_advanced"].sum()
    print(f"  Cluster {cluster_id}: {advanced_rate * 100:.1f}% ({advanced_count}/{len(cluster_data)})")

# --- 10. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó ---
df.to_csv("clustered_users.csv", index=False)

print("\n" + "=" * 70)
print("‚úì –ì–û–¢–û–í–û!")
print("=" * 70)
print("\n–ó–±–µ—Ä–µ–∂–µ–Ω—ñ —Ñ–∞–π–ª–∏:")
print("  - clustered_users.csv (–∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –∑ –Ω–æ–º–µ—Ä–∞–º–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)")
print("  - cluster_profiles.csv (—Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ñ—ñ—á –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö)")
print("  - elbow_plot.png (–≥—Ä–∞—Ñ—ñ–∫ –º–µ—Ç–æ–¥—É –ª—ñ–∫—Ç—è)")
print("  - silhouette_plot.png (–≥—Ä–∞—Ñ—ñ–∫ silhouette score)")
print("  - clusters_pca.png (–≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤)")

# --- 11. –ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ú–û–î–ï–õ–Ü –¢–ê SCALER ---
print("\n" + "=" * 70)
print("–ó–ë–ï–†–ï–ñ–ï–ù–ù–Ø –ú–û–î–ï–õ–Ü")
print("=" * 70)

# –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –º–æ–¥–µ–ª—å, scaler —Ç–∞ max_ttfm_value
with open("clustering_model.pkl", "wb") as f:
    pickle.dump({
        'model': model,
        'scaler': scaler,
        'features': features,
        'max_ttfm': max_ttfm_value,
        'best_k': best_k
    }, f)

print("‚úì –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É —Ñ–∞–π–ª clustering_model.pkl")

# =============================================================================
# --- 12. –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø –ù–û–í–ò–• –ö–û–†–ò–°–¢–£–í–ê–ß–Ü–í ---
# =============================================================================

print("\n" + "=" * 70)
print("–ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–Ø –ù–û–í–ò–• –ö–û–†–ò–°–¢–£–í–ê–ß–Ü–í")
print("=" * 70)

try:
    # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
    new_users_df = pd.read_csv("na_user_features.csv")
    print(f"\n‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(new_users_df)} –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤")

    # –û–±—Ä–æ–±–∫–∞ NULL –∑–Ω–∞—á–µ–Ω—å —É time_to_first_message_sec
    null_count_new = new_users_df["time_to_first_message_sec"].isna().sum()

    if null_count_new > 0:
        print(f"\n–û–±—Ä–æ–±–∫–∞ {null_count_new} NULL –∑–Ω–∞—á–µ–Ω—å —É –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤")
        replacement_value_new = 2 * max_ttfm_value
        new_users_df["time_to_first_message_sec"].fillna(replacement_value_new, inplace=True)
        print(f"‚úì NULL –∑–Ω–∞—á–µ–Ω–Ω—è –∑–∞–º—ñ–Ω–µ–Ω—ñ –Ω–∞: {replacement_value_new:.2f}")

    # –í–∏–±–∏—Ä–∞—î–º–æ —Ç—ñ–ª—å–∫–∏ –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ñ—ñ—á—ñ
    X_new = new_users_df[features].copy()

    # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –≤—Å—ñ—Ö –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö —Ñ—ñ—á
    missing_features = set(features) - set(X_new.columns)
    if missing_features:
        print(f"\n‚ö† –ü–û–ú–ò–õ–ö–ê: –í—ñ–¥—Å—É—Ç–Ω—ñ —Ñ—ñ—á—ñ: {missing_features}")
    else:
        # –ú–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è –Ω–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ç–æ–π —Å–∞–º–∏–π scaler
        X_new_scaled = scaler.transform(X_new)
        print("‚úì –ù–æ–≤—ñ –¥–∞–Ω—ñ –≤—ñ–¥–º–∞—Å—à—Ç–∞–±–æ–≤–∞–Ω—ñ")

        # –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
        new_users_df["cluster"] = model.predict(X_new_scaled)
        print("‚úì –ö–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –≤–∏–∫–æ–Ω–∞–Ω–∞")

        # –í–∏–∑–Ω–∞—á–∞—î–º–æ —è–∫—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ –º–∞—é—Ç—å –≤–∏—Å–æ–∫–∏–π churn rate
        print("\n" + "=" * 70)
        print("–ü–ï–†–ï–î–ë–ê–ß–ï–ù–ù–Ø CHURN –î–õ–Ø –ù–û–í–ò–• –ö–û–†–ò–°–¢–£–í–ê–ß–Ü–í")
        print("=" * 70)

        # –û—Ç—Ä–∏–º—É—î–º–æ churn rate –∫–æ–∂–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞ –∑ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö
        cluster_churn_rates = df.groupby("cluster")["is_churned"].mean()
        print("\nChurn rate –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö (–∑ –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö):")
        for cluster_id in sorted(cluster_churn_rates.index):
            rate = cluster_churn_rates[cluster_id]
            print(f"  Cluster {cluster_id}: {rate * 100:.1f}%")

        # –ü—Ä–∏—Å–≤–æ—é—î–º–æ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è churn –Ω–∞ –æ—Å–Ω–æ–≤—ñ –∫–ª–∞—Å—Ç–µ—Ä–∞
        new_users_df["predicted_churn_probability"] = new_users_df["cluster"].map(cluster_churn_rates)
        new_users_df["predicted_churned"] = (new_users_df["predicted_churn_probability"] > 0.5).astype(int)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö –¥–ª—è –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
        print("\n–†–æ–∑–ø–æ–¥—ñ–ª –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö:")
        new_cluster_counts = new_users_df["cluster"].value_counts().sort_index()
        for cluster_id, count in new_cluster_counts.items():
            percentage = count / len(new_users_df) * 100
            churn_prob = cluster_churn_rates[cluster_id] * 100
            risk_level = "üî¥ –í–ò–°–û–ö–ò–ô" if churn_prob > 70 else "üü° –°–ï–†–ï–î–ù–Ü–ô" if churn_prob > 40 else "üü¢ –ù–ò–ó–¨–ö–ò–ô"
            print(
                f"  Cluster {cluster_id}: {count} –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ ({percentage:.1f}%) - {risk_level} —Ä–∏–∑–∏–∫ churn ({churn_prob:.1f}%)")

        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω—å
        predicted_churned_count = new_users_df["predicted_churned"].sum()
        predicted_churned_pct = predicted_churned_count / len(new_users_df) * 100

        print("\n" + "=" * 70)
        print("üìä –ó–ê–ì–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–ï–†–ï–î–ë–ê–ß–ï–ù–¨")
        print("=" * 70)
        print(f"\n–í—Å—å–æ–≥–æ –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤: {len(new_users_df)}")
        print(f"–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ churned: {predicted_churned_count} ({predicted_churned_pct:.1f}%)")
        print(f"–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–æ active: {len(new_users_df) - predicted_churned_count} ({100 - predicted_churned_pct:.1f}%)")

        # –¢–æ–ø –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∑ –Ω–∞–π–≤–∏—â–∏–º —Ä–∏–∑–∏–∫–æ–º churn
        print("\n" + "=" * 70)
        print("‚ö†Ô∏è –¢–û–ü-10 –ö–û–†–ò–°–¢–£–í–ê–ß–Ü–í –ó –ù–ê–ô–í–ò–©–ò–ú –†–ò–ó–ò–ö–û–ú CHURN")
        print("=" * 70)

        top_risk_users = new_users_df.nlargest(10, 'predicted_churn_probability')

        if 'user_id' in new_users_df.columns:
            display_cols = ['user_id', 'cluster', 'predicted_churn_probability', 'predicted_churned']
        else:
            display_cols = ['cluster', 'predicted_churn_probability', 'predicted_churned']
            top_risk_users = top_risk_users.reset_index()
            display_cols = ['index'] + display_cols

        print("\n" + top_risk_users[display_cols].to_string(index=False))

        # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        print("\n" + "=" * 70)
        print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–Ü–á –ü–û –†–û–ë–û–¢–Ü –ó –ö–õ–ê–°–¢–ï–†–ê–ú–ò")
        print("=" * 70)

        for cluster_id in sorted(new_users_df["cluster"].unique()):
            cluster_data = new_users_df[new_users_df["cluster"] == cluster_id]
            churn_prob = cluster_churn_rates[cluster_id] * 100
            count = len(cluster_data)

            print(f"\nüîπ Cluster {cluster_id} ({count} –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, churn risk: {churn_prob:.1f}%):")

            if churn_prob > 70:
                print("   üî¥ –ö–†–ò–¢–ò–ß–ù–ò–ô –†–ò–ó–ò–ö - –¢–µ—Ä–º—ñ–Ω–æ–≤—ñ –¥—ñ—ó:")
                print("      ‚Ä¢ –ü–µ—Ä—Å–æ–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –ø—Ä–æ–ø–æ–∑–∏—Ü—ñ—ó —Ç–∞ –∑–Ω–∏–∂–∫–∏")
                print("      ‚Ä¢ –ü—Ä—ñ–æ—Ä–∏—Ç–µ—Ç–Ω–∞ –ø—ñ–¥—Ç—Ä–∏–º–∫–∞")
                print("      ‚Ä¢ –î–æ—Å–ª—ñ–¥–∂–µ–Ω–Ω—è –ø—Ä–∏—á–∏–Ω –Ω–µ–∑–∞–¥–æ–≤–æ–ª–µ–Ω–Ω—è")
            elif churn_prob > 40:
                print("   üü° –°–ï–†–ï–î–ù–Ü–ô –†–ò–ó–ò–ö - –ü—Ä–µ–≤–µ–Ω—Ç–∏–≤–Ω—ñ –∑–∞—Ö–æ–¥–∏:")
                print("      ‚Ä¢ Engagement campaigns")
                print("      ‚Ä¢ –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è onboarding")
                print("      ‚Ä¢ –ó–±—ñ—Ä feedback")
            else:
                print("   üü¢ –ù–ò–ó–¨–ö–ò–ô –†–ò–ó–ò–ö - –ü—ñ–¥—Ç—Ä–∏–º–∫–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ:")
                print("      ‚Ä¢ –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞ –∫–æ–º—É–Ω—ñ–∫–∞—Ü—ñ—è")
                print("      ‚Ä¢ Upsell –º–æ–∂–ª–∏–≤–æ—Å—Ç—ñ")
                print("      ‚Ä¢ Community building")

        # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç PCA –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        coords_new = pca.transform(X_new_scaled)
        new_users_df["pca1"] = coords_new[:, 0]
        new_users_df["pca2"] = coords_new[:, 1]

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –∫–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω–∏—Ö –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤
        new_users_df.to_csv("classified_new_users.csv", index=False)
        print("\n‚úì –ö–ª–∞—Å–∏—Ñ—ñ–∫–æ–≤–∞–Ω—ñ –Ω–æ–≤—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ –∑–±–µ—Ä–µ–∂–µ–Ω—ñ —É classified_new_users.csv")

        # --- –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –Ω–∞ —Ñ–æ–Ω—ñ —ñ—Å–Ω—É—é—á–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ ---
        print("\n" + "=" * 70)
        print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –ù–û–í–ò–• –ö–û–†–ò–°–¢–£–í–ê–ß–Ü–í")
        print("=" * 70)

        plt.figure(figsize=(14, 10))

        # –°–ø–æ—á–∞—Ç–∫—É –º–∞–ª—é—î–º–æ —ñ—Å–Ω—É—é—á—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏ (–Ω–∞–ø—ñ–≤–ø—Ä–æ–∑–æ—Ä—ñ)
        for i in range(best_k):
            cluster_data = df[df["cluster"] == i]
            plt.scatter(cluster_data["pca1"], cluster_data["pca2"],
                        c=[colors[i]], label=f"Cluster {i} (existing)",
                        alpha=0.3, s=30, edgecolors='none')

        # –ü–æ—Ç—ñ–º –º–∞–ª—é—î–º–æ –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ (—è—Å–∫—Ä–∞–≤—ñ)
        for i in range(best_k):
            new_cluster_data = new_users_df[new_users_df["cluster"] == i]
            if len(new_cluster_data) > 0:
                plt.scatter(new_cluster_data["pca1"], new_cluster_data["pca2"],
                            c=[colors[i]], label=f"Cluster {i} (NEW)",
                            alpha=0.9, s=100, marker='*', edgecolors='black', linewidths=1)

        plt.xlabel(f"PC1 ({explained_var[0] * 100:.1f}% variance)", fontsize=12)
        plt.ylabel(f"PC2 ({explained_var[1] * 100:.1f}% variance)", fontsize=12)
        plt.title("New Users Classification (PCA Visualization)", fontsize=14, fontweight='bold')
        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
        plt.grid(True, alpha=0.3)
        plt.tight_layout()
        plt.savefig("new_users_classification.png", dpi=200, bbox_inches='tight')
        plt.show()

        print("‚úì –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É new_users_classification.png")

        # --- –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –ø—Ä–æ—Ñ—ñ–ª—ñ–≤ –Ω–æ–≤–∏—Ö —Ç–∞ —ñ—Å–Ω—É—é—á–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ ---
        print("\n" + "=" * 70)
        print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ü–†–û–§–Ü–õ–Ü–í")
        print("=" * 70)

        new_cluster_profile = new_users_df.groupby("cluster")[features].mean()

        print("\n–°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è —Ñ—ñ—á –¥–ª—è –ù–û–í–ò–• –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤:")
        print(new_cluster_profile.round(2))

        print("\n–ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ —ñ—Å–Ω—É—é—á–∏–º–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞–º–∏:")
        for cluster_id in sorted(new_users_df["cluster"].unique()):
            print(f"\n--- Cluster {cluster_id} ---")
            if cluster_id in cluster_profile.index:
                comparison = pd.DataFrame({
                    'Existing': cluster_profile.loc[cluster_id],
                    'New': new_cluster_profile.loc[cluster_id],
                    'Diff %': ((new_cluster_profile.loc[cluster_id] - cluster_profile.loc[cluster_id]) /
                               (cluster_profile.loc[cluster_id] + 0.001) * 100)
                }).round(2)
                print(comparison)
            else:
                print(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_id} –Ω–µ –±—É–≤ —É –Ω–∞–≤—á–∞–ª—å–Ω–∏—Ö –¥–∞–Ω–∏—Ö")

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –ø–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª—ñ–∑—É
        comparison_summary = pd.DataFrame({
            'existing_users': df['cluster'].value_counts().sort_index(),
            'new_users': new_users_df['cluster'].value_counts().reindex(
                df['cluster'].unique(), fill_value=0
            )
        })
        comparison_summary['percentage_change'] = (
            (comparison_summary['new_users'] / comparison_summary['existing_users'] * 100)
        ).round(1)

        print("\n" + "=" * 70)
        print("–§–Ü–ù–ê–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê")
        print("=" * 70)
        print("\n–†–æ–∑–ø–æ–¥—ñ–ª –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤:")
        print(comparison_summary)

        comparison_summary.to_csv("classification_comparison.csv")
        print("\n‚úì –ü–æ—Ä—ñ–≤–Ω—è–ª—å–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É classification_comparison.csv")

except FileNotFoundError:
    print("\n‚ö† –§–∞–π–ª na_user_features.csv –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")
    print("–ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—é –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤")
except Exception as e:
    print(f"\n‚ö† –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó –Ω–æ–≤–∏—Ö –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤: {e}")

# =============================================================================
# --- 13. –û–¶–Ü–ù–ö–ê –Ø–ö–û–°–¢–Ü –ú–û–î–ï–õ–Ü ---
# =============================================================================

print("\n" + "=" * 70)
print("–û–¶–Ü–ù–ö–ê –Ø–ö–û–°–¢–Ü –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–Ü–á")
print("=" * 70)

from sklearn.metrics import (
    silhouette_score,
    davies_bouldin_score,
    calinski_harabasz_score,
    confusion_matrix,
    classification_report,
    accuracy_score,
    precision_recall_fscore_support
)

# --- 13.1. –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó (unsupervised) ---
print("\nüîç –í–ù–£–¢–†–Ü–®–ù–Ü –ú–ï–¢–†–ò–ö–ò –Ø–ö–û–°–¢–Ü –ö–õ–ê–°–¢–ï–†–Ü–í:")
print("-" * 70)

# Silhouette Score (–≤–∂–µ –æ–±—á–∏—Å–ª—é–≤–∞–ª–∏ —Ä–∞–Ω—ñ—à–µ, –∞–ª–µ –ø–æ–∫–∞–∂–µ–º–æ —â–µ —Ä–∞–∑)
silhouette = silhouette_score(X_scaled, df["cluster"])
print(f"Silhouette Score: {silhouette:.4f}")
print(f"  –î—ñ–∞–ø–∞–∑–æ–Ω: [-1, 1], –∫—Ä–∞—â–µ > 0.5")
print(f"  –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è: {'‚úì –î–æ–±—Ä–µ' if silhouette > 0.5 else '‚ö† –°–µ—Ä–µ–¥–Ω—å–æ' if silhouette > 0.25 else '‚úó –ü–æ–≥–∞–Ω–æ'}")

# Davies-Bouldin Index (–º–µ–Ω—à–µ = –∫—Ä–∞—â–µ)
davies_bouldin = davies_bouldin_score(X_scaled, df["cluster"])
print(f"\nDavies-Bouldin Index: {davies_bouldin:.4f}")
print(f"  –î—ñ–∞–ø–∞–∑–æ–Ω: [0, ‚àû], –∫—Ä–∞—â–µ < 1.0")
print(f"  –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è: {'‚úì –î–æ–±—Ä–µ' if davies_bouldin < 1.0 else '‚ö† –°–µ—Ä–µ–¥–Ω—å–æ' if davies_bouldin < 2.0 else '‚úó –ü–æ–≥–∞–Ω–æ'}")

# Calinski-Harabasz Score (–±—ñ–ª—å—à–µ = –∫—Ä–∞—â–µ)
calinski = calinski_harabasz_score(X_scaled, df["cluster"])
print(f"\nCalinski-Harabasz Score: {calinski:.2f}")
print(f"  –î—ñ–∞–ø–∞–∑–æ–Ω: [0, ‚àû], –∫—Ä–∞—â–µ > 100")
print(f"  –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è: {'‚úì –î–æ–±—Ä–µ' if calinski > 100 else '‚ö† –°–µ—Ä–µ–¥–Ω—å–æ' if calinski > 50 else '‚úó –ü–æ–≥–∞–Ω–æ'}")

# Inertia (—Å—É–º–∞ –∫–≤–∞–¥—Ä–∞—Ç—ñ–≤ –≤—ñ–¥—Å—Ç–∞–Ω–µ–π –¥–æ —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤)
inertia = model.inertia_
print(f"\nInertia: {inertia:.2f}")
print(f"  –ú–µ–Ω—à–µ = –∫—Ä–∞—â–µ (–∞–ª–µ –∑–∞–ª–µ–∂–∏—Ç—å –≤—ñ–¥ —Ä–æ–∑–º—ñ—Ä—É –¥–∞–Ω–∏—Ö)")

# --- 13.2. –ó–æ–≤–Ω—ñ—à–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ (supervised) - —è–∫—â–æ —î ground truth ---
print("\n" + "=" * 70)
print("–ó–û–í–ù–Ü–®–ù–Ü –ú–ï–¢–†–ò–ö–ò (–∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º is_churned —è–∫ proxy)")
print("=" * 70)

# –í–∏–∫–æ—Ä–∏—Å—Ç–∞—î–º–æ is_churned —è–∫ ground truth –¥–ª—è –æ—Ü—ñ–Ω–∫–∏
# –¶–µ –Ω–µ —ñ–¥–µ–∞–ª—å–Ω–æ, –∞–ª–µ –¥–∞—î —É—è–≤–ª–µ–Ω–Ω—è –ø—Ä–æ –∑–¥–∞—Ç–Ω—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –≤–∏—è–≤–ª—è—Ç–∏ churn
print("\n‚ö† –í–ê–ñ–õ–ò–í–û: –¶—ñ –º–µ—Ç—Ä–∏–∫–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å 'is_churned' —è–∫ –ø—Ä–∏–±–ª–∏–∑–Ω–∏–π")
print("   ground truth. –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è —î unsupervised, —Ç–æ–º—É —Ü–µ –ª–∏—à–µ –æ—Ä—ñ—î–Ω—Ç–æ–≤–Ω—ñ –æ—Ü—ñ–Ω–∫–∏.")

# –°—Ç–≤–æ—Ä—é—î–º–æ "–ø—Å–µ–≤–¥–æ-–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—é": –∫–ª–∞—Å—Ç–µ—Ä–∏ –∑ churn > 50% = "churned"
cluster_churn_rates = df.groupby("cluster")["is_churned"].mean()
high_churn_clusters = cluster_churn_rates[cluster_churn_rates > 0.5].index.tolist()

df["predicted_churn"] = df["cluster"].isin(high_churn_clusters).astype(int)
y_true = df["is_churned"]
y_pred = df["predicted_churn"]

print(f"\n–ö–ª–∞—Å—Ç–µ—Ä–∏ –∑ –≤–∏—Å–æ–∫–∏–º churn rate (>{50}%): {high_churn_clusters}")

# Confusion Matrix
conf_matrix = confusion_matrix(y_true, y_pred)
print("\nüìä Confusion Matrix:")
print("                Predicted")
print("                Not Churn  Churn")
print(f"Actual Not Churn    {conf_matrix[0, 0]:<6}  {conf_matrix[0, 1]:<6}")
print(f"Actual Churn        {conf_matrix[1, 0]:<6}  {conf_matrix[1, 1]:<6}")

# Accuracy, Precision, Recall, F1
accuracy = accuracy_score(y_true, y_pred)
precision, recall, f1, _ = precision_recall_fscore_support(
    y_true, y_pred, average='binary', zero_division=0
)

print("\nüìà –ú–ï–¢–†–ò–ö–ò –ö–õ–ê–°–ò–§–Ü–ö–ê–¶–Ü–á (churn prediction):")
print("-" * 70)
print(f"Accuracy:  {accuracy:.4f} ({accuracy * 100:.2f}%)")
print(f"Precision: {precision:.4f} ({precision * 100:.2f}%)")
print(f"  - –ó —É—Å—ñ—Ö –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–∏—Ö churned, —Å–∫—ñ–ª—å–∫–∏ –Ω–∞—Å–ø—Ä–∞–≤–¥—ñ churned")
print(f"Recall:    {recall:.4f} ({recall * 100:.2f}%)")
print(f"  - –ó —É—Å—ñ—Ö —Å–ø—Ä–∞–≤–∂–Ω—ñ—Ö churned, —Å–∫—ñ–ª—å–∫–∏ –º–∏ –≤–∏—è–≤–∏–ª–∏")
print(f"F1-Score:  {f1:.4f}")
print(f"  - –ì–∞—Ä–º–æ–Ω—ñ–π–Ω–µ —Å–µ—Ä–µ–¥–Ω—î precision —ñ recall")

# –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç
print("\nüìã –î–ï–¢–ê–õ–¨–ù–ò–ô CLASSIFICATION REPORT:")
print("-" * 70)
print(classification_report(y_true, y_pred,
                            target_names=['Not Churned', 'Churned'],
                            zero_division=0))

# --- 13.3. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ –±–∞–∑–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é (baseline) ---
print("\n" + "=" * 70)
print("–ü–û–†–Ü–í–ù–Ø–ù–ù–Ø –ó BASELINE")
print("=" * 70)

# Baseline 1: –ó–∞–≤–∂–¥–∏ –ø–µ—Ä–µ–¥–±–∞—á–∞—Ç–∏ most frequent class
most_frequent_class = y_true.mode()[0]
baseline_pred_1 = np.full(len(y_true), most_frequent_class)
baseline_acc_1 = accuracy_score(y_true, baseline_pred_1)

# Baseline 2: –í–∏–ø–∞–¥–∫–æ–≤–µ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
np.random.seed(42)
baseline_pred_2 = np.random.randint(0, 2, len(y_true))
baseline_acc_2 = accuracy_score(y_true, baseline_pred_2)

print(f"\nBaseline 1 (most frequent class): {baseline_acc_1:.4f} ({baseline_acc_1 * 100:.2f}%)")
print(f"Baseline 2 (random prediction):   {baseline_acc_2:.4f} ({baseline_acc_2 * 100:.2f}%)")
print(f"Our Model:                         {accuracy:.4f} ({accuracy * 100:.2f}%)")

improvement_1 = ((accuracy - baseline_acc_1) / baseline_acc_1 * 100) if baseline_acc_1 > 0 else 0
improvement_2 = ((accuracy - baseline_acc_2) / baseline_acc_2 * 100) if baseline_acc_2 > 0 else 0

print(f"\nüìä –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤—ñ–¥–Ω–æ—Å–Ω–æ baseline 1: {improvement_1:+.2f}%")
print(f"üìä –ü–æ–∫—Ä–∞—â–µ–Ω–Ω—è –≤—ñ–¥–Ω–æ—Å–Ω–æ baseline 2: {improvement_2:+.2f}%")

# --- 13.4. –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –≤—Å—ñ—Ö –º–µ—Ç—Ä–∏–∫ ---
metrics_summary = pd.DataFrame({
    'Metric': [
        'Silhouette Score',
        'Davies-Bouldin Index',
        'Calinski-Harabasz Score',
        'Inertia',
        'Churn Accuracy',
        'Churn Precision',
        'Churn Recall',
        'Churn F1-Score',
        'Baseline Accuracy (most frequent)',
        'Baseline Accuracy (random)',
        'Improvement vs Baseline 1 (%)',
        'Improvement vs Baseline 2 (%)'
    ],
    'Value': [
        silhouette,
        davies_bouldin,
        calinski,
        inertia,
        accuracy,
        precision,
        recall,
        f1,
        baseline_acc_1,
        baseline_acc_2,
        improvement_1,
        improvement_2
    ]
})

metrics_summary.to_csv("model_performance_metrics.csv", index=False)
print("\n‚úì –ú–µ—Ç—Ä–∏–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É model_performance_metrics.csv")

# --- 13.6. –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ—Ç—Ä–∏–∫ ---
print("\n" + "=" * 70)
print("–í–Ü–ó–£–ê–õ–Ü–ó–ê–¶–Ü–Ø –ú–ï–¢–†–ò–ö")
print("=" * 70)

fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Confusion Matrix –¥–ª—è Churn
ax1 = axes[0, 0]
im1 = ax1.imshow(conf_matrix, cmap='Blues', aspect='auto')
ax1.set_xticks([0, 1])
ax1.set_yticks([0, 1])
ax1.set_xticklabels(['Not Churned', 'Churned'])
ax1.set_yticklabels(['Not Churned', 'Churned'])
ax1.set_xlabel('Predicted')
ax1.set_ylabel('Actual')
ax1.set_title('Confusion Matrix (Churn)', fontweight='bold')

# –î–æ–¥–∞—î–º–æ —á–∏—Å–ª–∞ –≤ –∫–ª—ñ—Ç–∏–Ω–∫–∏
for i in range(2):
    for j in range(2):
        text = ax1.text(j, i, conf_matrix[i, j],
                        ha="center", va="center", color="black", fontsize=14)

plt.colorbar(im1, ax=ax1)

# 2. –ú–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó
ax2 = axes[0, 1]
metrics_to_plot = ['Accuracy', 'Precision', 'Recall', 'F1-Score']
churn_values = [accuracy, precision, recall, f1]

x = np.arange(len(metrics_to_plot))
bars = ax2.bar(x, churn_values, color=['#2ecc71', '#3498db', '#e74c3c', '#f39c12'])

ax2.set_xlabel('Metrics')
ax2.set_ylabel('Score')
ax2.set_title('Churn Prediction Performance', fontweight='bold')
ax2.set_xticks(x)
ax2.set_xticklabels(metrics_to_plot, rotation=45, ha='right')
ax2.set_ylim([0, 1])
ax2.grid(axis='y', alpha=0.3)

# –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—á–∏–∫–∏
for bar in bars:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width() / 2., height,
             f'{height:.3f}',
             ha='center', va='bottom', fontsize=10, fontweight='bold')

# 3. –í–Ω—É—Ç—Ä—ñ—à–Ω—ñ –º–µ—Ç—Ä–∏–∫–∏ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó
ax3 = axes[1, 0]
internal_metrics = ['Silhouette\nScore', 'Davies-Bouldin\nIndex', 'Calinski-Harabasz\nScore (√ó0.01)']
internal_values = [silhouette, davies_bouldin, calinski / 100]  # –ú–∞—Å—à—Ç–∞–±—É—î–º–æ CH –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó

bars3 = ax3.bar(internal_metrics, internal_values, color=['green', 'orange', 'purple'])
ax3.set_ylabel('Score')
ax3.set_title('Internal Clustering Metrics', fontweight='bold')
ax3.grid(axis='y', alpha=0.3)

# –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è
for i, bar in enumerate(bars3):
    height = bar.get_height()
    actual_value = [silhouette, davies_bouldin, calinski][i]
    ax3.text(bar.get_x() + bar.get_width() / 2., height,
             f'{actual_value:.2f}',
             ha='center', va='bottom', fontsize=10)

# 4. –ü–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è –∑ Baseline
ax4 = axes[1, 1]
comparison_labels = ['Baseline\n(most frequent)', 'Baseline\n(random)', 'Our Model']
comparison_values = [baseline_acc_1, baseline_acc_2, accuracy]
colors_comp = ['lightgray', 'lightgray', 'green']

bars4 = ax4.bar(comparison_labels, comparison_values, color=colors_comp)
ax4.set_ylabel('Accuracy')
ax4.set_title('Model vs Baseline Comparison', fontweight='bold')
ax4.set_ylim([0, 1])
ax4.grid(axis='y', alpha=0.3)

# –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è
for bar in bars4:
    height = bar.get_height()
    ax4.text(bar.get_x() + bar.get_width() / 2., height,
             f'{height:.3f}',
             ha='center', va='bottom', fontsize=10)

plt.tight_layout()
plt.savefig("model_performance_analysis.png", dpi=200, bbox_inches='tight')
plt.show()

print("‚úì –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É model_performance_analysis.png")

# --- 13.7. –§—ñ–Ω–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç ---
print("\n" + "=" * 70)
print("üìä –§–Ü–ù–ê–õ–¨–ù–ò–ô –ó–í–Ü–¢ –ü–†–û –Ø–ö–Ü–°–¢–¨ –ú–û–î–ï–õ–Ü")
print("=" * 70)

print(f"""
üéØ –ó–ê–ì–ê–õ–¨–ù–ê –û–¶–Ü–ù–ö–ê –ö–õ–ê–°–¢–ï–†–ò–ó–ê–¶–Ü–á:
{'=' * 70}
–ú–æ–¥–µ–ª—å —Å—Ç–≤–æ—Ä–∏–ª–∞ {best_k} –∫–ª–∞—Å—Ç–µ—Ä–∏ –∑ –Ω–∞—Å—Ç—É–ø–Ω–∏–º–∏ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞–º–∏:

üìå –í–ù–£–¢–†–Ü–®–ù–Ü –ú–ï–¢–†–ò–ö–ò (—è–∫—ñ—Å—Ç—å —Ä–æ–∑–¥—ñ–ª–µ–Ω–Ω—è):
   ‚Ä¢ Silhouette Score: {silhouette:.4f} {'‚úì –î–æ–±—Ä–µ' if silhouette > 0.5 else '‚ö† –°–µ—Ä–µ–¥–Ω—å–æ'}
   ‚Ä¢ Davies-Bouldin Index: {davies_bouldin:.4f} {'‚úì –î–æ–±—Ä–µ' if davies_bouldin < 1.0 else '‚ö† –°–µ—Ä–µ–¥–Ω—å–æ'}
   ‚Ä¢ Calinski-Harabasz Score: {calinski:.2f} {'‚úì –î–æ–±—Ä–µ' if calinski > 100 else '‚ö† –°–µ—Ä–µ–¥–Ω—å–æ'}

üìå –ü–†–û–ì–ù–û–ó–£–í–ê–ù–ù–Ø CHURN:
   ‚Ä¢ Accuracy: {accuracy * 100:.2f}% (–Ω–∞ {improvement_1:+.1f}% –∫—Ä–∞—â–µ baseline)
   ‚Ä¢ Precision: {precision * 100:.2f}% (—Ç–æ—á–Ω—ñ—Å—Ç—å –≤–∏—è–≤–ª–µ–Ω–Ω—è churned users)
   ‚Ä¢ Recall: {recall * 100:.2f}% (–ø–æ–≤–Ω–æ—Ç–∞ –≤–∏—è–≤–ª–µ–Ω–Ω—è churned users)
   ‚Ä¢ F1-Score: {f1:.4f}

üí° –í–ò–°–ù–û–í–û–ö:
   –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è {'—É—Å–ø—ñ—à–Ω–æ' if silhouette > 0.5 and davies_bouldin < 1.0 else '–ø–æ–º—ñ—Ä–Ω–æ'} —Ä–æ–∑–¥—ñ–ª—è—î –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –Ω–∞ —Å–µ–≥–º–µ–Ω—Ç–∏.
   –ú–æ–¥–µ–ª—å {'–∑–Ω–∞—á–Ω–æ' if improvement_1 > 10 else '–ø–æ–º—ñ—Ä–Ω–æ' if improvement_1 > 0 else '–Ω–µ'} –ø–µ—Ä–µ–≤–µ—Ä—à—É—î baseline –ø—ñ–¥—Ö–æ–¥–∏.
   {'–í–∏—Å–æ–∫–∞ —Ç–æ—á–Ω—ñ—Å—Ç—å (precision) —ñ –≤—ñ–¥–º—ñ–Ω–Ω–∏–π recall –≤–∫–∞–∑—É—é—Ç—å –Ω–∞ –Ω–∞–¥—ñ–π–Ω—ñ—Å—Ç—å –≤–∏—è–≤–ª–µ–Ω–Ω—è churn.' if precision > 0.8 and recall > 0.9 else ''}
""")

print("\n" + "=" * 70)
print("‚úì –í–°–ï –ó–ê–í–î–ê–ù–ù–Ø –í–ò–ö–û–ù–ê–ù–û!")
print("=" * 70)


