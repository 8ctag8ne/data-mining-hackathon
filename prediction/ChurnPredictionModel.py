import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import RobustScaler
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, confusion_matrix, roc_auc_score,
    roc_curve, precision_recall_curve, f1_score, accuracy_score
)
import warnings

warnings.filterwarnings('ignore')

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")


class ChurnPredictionModel:
    def __init__(self, filepath, use_cleaned=False, use_categories=True):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –º–æ–¥–µ–ª—ñ –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è churn

        Parameters:
        -----------
        filepath : str
            –®–ª—è—Ö –¥–æ CSV —Ñ–∞–π–ª—É
        use_cleaned : bool
            True - –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ —á–∞—Å—Ç–∫–æ–≤–æ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç (–∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏)
            False - –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç
        use_categories : bool
            True - –≥—Ä—É–ø—É–≤–∞—Ç–∏ –æ–∑–Ω–∞–∫–∏ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏ (–º–µ–Ω—à–µ –æ–∑–Ω–∞–∫)
            False - multi-hot encoding –¥–ª—è –∫–æ–∂–Ω–æ–≥–æ —É–Ω—ñ–∫–∞–ª—å–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è (–±—ñ–ª—å—à–µ –æ–∑–Ω–∞–∫)
        """
        self.filepath = filepath
        self.use_cleaned = use_cleaned
        self.use_categories = use_categories
        self.df_original = None
        self.df_processed = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.scaler = RobustScaler()
        self.model = None
        self.feature_importance = None

    def load_and_prepare_data(self):
        """–ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö"""
        print("=" * 70)
        print("–ö–†–û–ö 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –ø—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö")
        print("=" * 70)

        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è
        self.df_original = pd.read_csv(self.filepath)
        print(f"‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.df_original)} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"‚úì –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–æ–∫: {len(self.df_original.columns)}")

        self.df_processed = self.df_original.copy()

        # –§—ñ–ª—å—Ç—Ä–∞—Ü—ñ—è: –∑–∞–ª–∏—à–∏—Ç–∏ —Ç—ñ–ª—å–∫–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∑ successful_purchase = 1
        if 'successful_purchase' in self.df_processed.columns:
            records_before = len(self.df_processed)
            self.df_processed = self.df_processed[self.df_processed['successful_purchase'] == 1].copy()
            records_after = len(self.df_processed)
            print(f"‚úì –í—ñ–¥—Ñ—ñ–ª—å—Ç—Ä–æ–≤–∞–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ –∑ successful_purchase = 1")
            print(f"  –ó–∞–ª–∏—à–∏–ª–æ—Å—å: {records_after} –∑ {records_before} ({records_after / records_before * 100:.1f}%)")

            # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–∫–∏ successful_purchase (–±—ñ–ª—å—à–µ –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞)
            self.df_processed.drop(columns=['successful_purchase'], inplace=True)
            self.df_processed.drop(columns=['answer_errors', 'messages_received'], inplace=True)

            print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–∫—É 'successful_purchase' (–≤—Å—ñ –∑–Ω–∞—á–µ–Ω–Ω—è = 1)")
        else:
            print("‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∞ 'successful_purchase' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞, –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Ñ—ñ–ª—å—Ç—Ä–∞—Ü—ñ—é")

        if 'likes' in self.df_processed.columns and 'dislikes' in self.df_processed.columns:
            print("‚úì –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫ like_rate —Ç–∞ dislike_rate...")

            likes = self.df_processed['likes']
            dislikes = self.df_processed['dislikes']
            total = likes + dislikes

            # –£–Ω–∏–∫–∞—î–º–æ –¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ 0
            self.df_processed['like_rate'] = likes / total.replace(0, np.nan)
            self.df_processed['dislike_rate'] = dislikes / total.replace(0, np.nan)

            # –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è NaN —É –≤–∏–ø–∞–¥–∫–∞—Ö total = 0
            self.df_processed['like_rate'].fillna(0, inplace=True)
            self.df_processed['dislike_rate'].fillna(0, inplace=True)

            # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å—Ç–∞—Ä–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
            self.df_processed.drop(columns=['likes', 'dislikes'], inplace=True)

            print("  ‚úì –ö–æ–ª–æ–Ω–∫–∏ 'likes' —Ç–∞ 'dislikes' –≤–∏–¥–∞–ª–µ–Ω–æ")
            print("  ‚úì –ù–æ–≤—ñ –∫–æ–ª–æ–Ω–∫–∏ 'like_rate' —Ç–∞ 'dislike_rate' –¥–æ–¥–∞–Ω–æ")
        else:
            print("‚ö†Ô∏è  –ö–æ–ª–æ–Ω–∫–∏ 'likes'/'dislikes' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—î–º–æ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è rate-—Ñ—ñ—á")

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        duplicates = self.df_processed.duplicated().sum()
        self.df_processed = self.df_processed.drop_duplicates()
        print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {duplicates}")

        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –±–∞–ª–∞–Ω—Å—É –∫–ª–∞—Å—ñ–≤
        churn_dist = self.df_processed['is_churned'].value_counts()
        churn_ratio = churn_dist[1] / len(self.df_processed) * 100
        print(f"\nüìä –†–æ–∑–ø–æ–¥—ñ–ª –∫–ª–∞—Å—ñ–≤:")
        print(f"   Churned (1): {churn_dist.get(1, 0)} ({churn_ratio:.2f}%)")
        print(f"   Active (0): {churn_dist.get(0, 0)} ({100 - churn_ratio:.2f}%)")

        if churn_ratio < 30 or churn_ratio > 70:
            print(f"   ‚ö†Ô∏è  –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤ –≤–∏—è–≤–ª–µ–Ω–æ! –ë—É–¥–µ –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω–æ class_weight='balanced'")

        # –Ø–∫—â–æ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è cleaned –≤–µ—Ä—Å—ñ—è, –≤–∏–∫–æ–Ω–∞—Ç–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—é
        if not self.use_cleaned:
            print("\n‚úì –í–∏–∫–æ–Ω—É—î—Ç—å—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è goals/interests/assistance...")
            self._categorize_features()
        else:
            print("‚úì –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –≤–∂–µ –æ–±—Ä–æ–±–ª–µ–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∑ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏")

        # –û–±—Ä–æ–±–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        self._handle_missing_values()

        print(f"‚úì –§—ñ–Ω–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∑–∞–ø–∏—Å—ñ–≤: {len(self.df_processed)}\n")

    def _categorize_features(self):
        """–ö–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ (—è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∏–π –¥–∞—Ç–∞—Å–µ—Ç)"""

        def split_values(s):
            if pd.isna(s) or s == "":
                return []
            return [x.strip().lower() for x in str(s).split(",") if x.strip()]

        # –û–±—Ä–æ–±–∫–∞ —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫, —è–∫—â–æ –≤–æ–Ω–∏ —î
        text_cols = ['user_goals', 'user_interests', 'user_assistance']

        for col in text_cols:
            if col in self.df_processed.columns:
                self.df_processed[f"{col}_list"] = self.df_processed[col].apply(split_values)

        # –ö–∞—Ç–µ–≥–æ—Ä—ñ—ó
        GOALS_CATEGORIES = {
            "goals_creative": ["content-creation", "essay-writing", "cooking"],
            "goals_professional": ["coding-assistance", "business-purposes"],
            "goals_personal_dev": ["education", "mental-health"],
            "goals_social_entertain": ["social-media", "entertainment"]
        }

        ASSISTANCE_CATEGORIES = {
            "assist_detailed": ["step-by-step", "detailed-explanation", "answer-with-explanation"],
            "assist_concise": ["direct-answer", "simplified-explanation"]
        }

        INTEREST_CATEGORIES = {
            "interest_creative_arts": ["arts", "music", "writing", "dance", "movies"],
            "interest_practical": ["diy", "cooking", "gardening"],
            "interest_outdoor": ["sports", "outdoor", "travel"],
            "interest_intellectual": ["reading", "learning", "history"],
            "interest_business_tech": ["business", "technology"],
            "interest_lifestyle": ["fashion", "family", "animals"]
        }

        def assign_categories(df, source_col, categories):
            if f"{source_col}_list" in df.columns:
                for new_col, group_values in categories.items():
                    df[new_col] = df[f"{source_col}_list"].apply(
                        lambda lst: int(any(item in lst for item in group_values))
                    )
            return df

        self.df_processed = assign_categories(self.df_processed, "user_goals", GOALS_CATEGORIES)
        self.df_processed = assign_categories(self.df_processed, "user_assistance", ASSISTANCE_CATEGORIES)
        self.df_processed = assign_categories(self.df_processed, "user_interests", INTEREST_CATEGORIES)

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —Å–∏—Ä–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
        cols_to_drop = [col for col in text_cols + [f"{c}_list" for c in text_cols]
                        if col in self.df_processed.columns]
        self.df_processed.drop(columns=cols_to_drop, inplace=True, errors='ignore')

    def _handle_missing_values(self):
        """–û–±—Ä–æ–±–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å"""
        missing_before = self.df_processed.isnull().sum().sum()

        if missing_before > 0:
            print(f"‚ö†Ô∏è  –ó–Ω–∞–π–¥–µ–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å: {missing_before}")

            # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ time_to_first_message_sec
            if 'time_to_first_message_sec' in self.df_processed.columns:
                no_messages = self.df_processed['time_to_first_message_sec'].isnull()
                if no_messages.sum() > 0:
                    max_time = self.df_processed['time_to_first_message_sec'].max()
                    self.df_processed.loc[no_messages, 'time_to_first_message_sec'] = max_time * 2
                    print(f"   ‚Ä¢ time_to_first_message_sec: –∑–∞–ø–æ–≤–Ω–µ–Ω–æ {no_messages.sum()} –∑–Ω–∞—á–µ–Ω—å")

            # –ó–∞–ø–æ–≤–Ω–µ–Ω–Ω—è –º–µ–¥—ñ–∞–Ω–æ—é –¥–ª—è —á–∏—Å–ª–æ–≤–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
            numeric_cols = self.df_processed.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if self.df_processed[col].isnull().sum() > 0:
                    self.df_processed[col].fillna(self.df_processed[col].median(), inplace=True)

            print(f"‚úì –í—Å—ñ –ø—Ä–æ–ø—É—â–µ–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–±—Ä–æ–±–ª–µ–Ω–æ")
        else:
            print("‚úì –ü—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å –Ω–µ –≤–∏—è–≤–ª–µ–Ω–æ")

    def feature_engineering(self):
        """–ö—Ä–æ–∫ 2: Feature engineering —Ç–∞ –≤—ñ–¥–±—ñ—Ä –æ–∑–Ω–∞–∫"""
        print("=" * 70)
        print("–ö–†–û–ö 2: Feature Engineering —Ç–∞ –≤—ñ–¥–±—ñ—Ä –æ–∑–Ω–∞–∫")
        print("=" * 70)

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
        cols_to_remove = ['user_id']
        if 'Unnamed: 0' in self.df_processed.columns:
            cols_to_remove.append('Unnamed: 0')

        self.df_processed.drop(columns=cols_to_remove, inplace=True, errors='ignore')
        print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏: {cols_to_remove}")

        # –í—ñ–¥–¥—ñ–ª–µ–Ω–Ω—è —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
        if 'is_churned' not in self.df_processed.columns:
            raise ValueError("–ö–æ–ª–æ–Ω–∫–∞ 'is_churned' –≤—ñ–¥—Å—É—Ç–Ω—è –≤ –¥–∞—Ç–∞—Å–µ—Ç—ñ!")

        X = self.df_processed.drop(columns=['is_churned'])
        y = self.df_processed['is_churned']

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫ –∑ –Ω–∏–∑—å–∫–æ—é –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—é
        low_variance_cols = []
        for col in X.select_dtypes(include=[np.number]).columns:
            if X[col].std() < 0.01:
                low_variance_cols.append(col)

        if low_variance_cols:
            X = X.drop(columns=low_variance_cols)
            print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ –∑ –Ω–∏–∑—å–∫–æ—é –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—é: {low_variance_cols}")

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –≤–∏—Å–æ–∫–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫ (—â–æ–± —É–Ω–∏–∫–Ω—É—Ç–∏ –º—É–ª—å—Ç–∏–∫–æ–ª—ñ–Ω–µ–∞—Ä–Ω–æ—Å—Ç—ñ)
        corr_matrix = X.corr().abs()
        upper_triangle = corr_matrix.where(
            np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)
        )

        high_corr_cols = []
        for column in upper_triangle.columns:
            if any(upper_triangle[column] > 0.95):
                high_corr_cols.append(column)

        if high_corr_cols:
            X = X.drop(columns=high_corr_cols)
            print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –≤–∏—Å–æ–∫–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ (r > 0.95): {high_corr_cols}")

        print(f"\n‚úì –§—ñ–Ω–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {len(X.columns)}")
        print(f"  –û–∑–Ω–∞–∫–∏: {list(X.columns)}\n")

        return X, y

    def split_and_scale(self, X, y, test_size=0.2, random_state=42):
        """–ö—Ä–æ–∫ 3: –ü–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö —Ç–∞ –º–∞—Å—à—Ç–∞–±—É–≤–∞–Ω–Ω—è"""
        print("=" * 70)
        print("–ö–†–û–ö 3: –ü–æ–¥—ñ–ª –¥–∞–Ω–∏—Ö —Ç–∞ –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è")
        print("=" * 70)

        # –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –Ω–∞ train/test –∑ —Å—Ç—Ä–∞—Ç–∏—Ñ—ñ–∫–∞—Ü—ñ—î—é
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=test_size, random_state=random_state, stratify=y
        )

        print(f"‚úì Train set: {len(self.X_train)} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"‚úì Test set: {len(self.X_test)} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"‚úì –°–ø—ñ–≤–≤—ñ–¥–Ω–æ—à–µ–Ω–Ω—è train/test: {(1 - test_size) * 100:.0f}% / {test_size * 100:.0f}%")

        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è (RobustScaler —Å—Ç—ñ–π–∫–∏–π –¥–æ –≤–∏–∫–∏–¥—ñ–≤)
        self.X_train = pd.DataFrame(
            self.scaler.fit_transform(self.X_train),
            columns=self.X_train.columns,
            index=self.X_train.index
        )

        self.X_test = pd.DataFrame(
            self.scaler.transform(self.X_test),
            columns=self.X_test.columns,
            index=self.X_test.index
        )

        print(f"‚úì –î–∞–Ω—ñ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é RobustScaler\n")

    def train_model(self, max_iter=1000, solver='lbfgs', class_weight='balanced'):
        """–ö—Ä–æ–∫ 4: –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ –ª–æ–≥—ñ—Å—Ç–∏—á–Ω–æ—ó —Ä–µ–≥—Ä–µ—Å—ñ—ó"""
        print("=" * 70)
        print("–ö–†–û–ö 4: –¢—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ")
        print("=" * 70)

        print(f"üöÄ –ü–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–æ–¥–µ–ª—ñ:")
        print(f"   ‚Ä¢ Solver: {solver} (—à–≤–∏–¥–∫–∏–π –¥–ª—è –º–∞–ª–∏—Ö/—Å–µ—Ä–µ–¥–Ω—ñ—Ö –¥–∞—Ç–∞—Å–µ—Ç—ñ–≤)")
        print(f"   ‚Ä¢ Max iterations: {max_iter}")
        print(f"   ‚Ä¢ Class weight: {class_weight} (–∫–æ–º–ø–µ–Ω—Å—É—î –¥–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—ñ–≤)")
        print(f"   ‚Ä¢ Penalty: L2 (ridge regression, —É–Ω–∏–∫–∞—î overfitting)\n")

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ç–∞ —Ç—Ä–µ–Ω—É–≤–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        self.model = LogisticRegression(
            max_iter=max_iter,
            solver=solver,
            class_weight=class_weight,
            random_state=42,
            penalty='l2',
            C=1.0  # Inverse of regularization strength
        )

        import time
        start_time = time.time()
        self.model.fit(self.X_train, self.y_train)
        training_time = time.time() - start_time

        print(f"‚úì –ú–æ–¥–µ–ª—å –Ω–∞—Ç—Ä–µ–Ω–æ–≤–∞–Ω–æ –∑–∞ {training_time:.4f} —Å–µ–∫—É–Ω–¥")

        # –í–∏—Ç—è–≥–Ω–µ–Ω–Ω—è –≤–∞–∂–ª–∏–≤–æ—Å—Ç—ñ –æ–∑–Ω–∞–∫
        self.feature_importance = pd.DataFrame({
            'feature': self.X_train.columns,
            'coefficient': self.model.coef_[0]
        }).sort_values('coefficient', key=abs, ascending=False)

        print(f"‚úì –ù–∞–π–≤–∞–∂–ª–∏–≤—ñ—à—ñ –æ–∑–Ω–∞–∫–∏ (—Ç–æ–ø-10):")
        for idx, row in self.feature_importance.head(10).iterrows():
            print(f"   {row['feature']:.<45} {row['coefficient']:>8.4f}")
        print()

    def cross_validate(self, cv=5):
        """–ö—Ä–æ–∫ 5: –ö—Ä–æ—Å-–≤–∞–ª—ñ–¥–∞—Ü—ñ—è"""
        print("=" * 70)
        print("–ö–†–û–ö 5: –ö—Ä–æ—Å-–≤–∞–ª—ñ–¥–∞—Ü—ñ—è")
        print("=" * 70)

        skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)

        # –û—Ü—ñ–Ω–∫–∞ –∑–∞ —Ä—ñ–∑–Ω–∏–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏
        cv_scores = {
            'accuracy': cross_val_score(self.model, self.X_train, self.y_train,
                                        cv=skf, scoring='accuracy'),
            'precision': cross_val_score(self.model, self.X_train, self.y_train,
                                         cv=skf, scoring='precision'),
            'recall': cross_val_score(self.model, self.X_train, self.y_train,
                                      cv=skf, scoring='recall'),
            'f1': cross_val_score(self.model, self.X_train, self.y_train,
                                  cv=skf, scoring='f1'),
            'roc_auc': cross_val_score(self.model, self.X_train, self.y_train,
                                       cv=skf, scoring='roc_auc')
        }

        print(f"üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ {cv}-fold –∫—Ä–æ—Å-–≤–∞–ª—ñ–¥–∞—Ü—ñ—ó:")
        for metric, scores in cv_scores.items():
            print(f"   {metric.upper():.<20} {scores.mean():.4f} (¬±{scores.std():.4f})")
        print()

        return cv_scores

    def evaluate_model(self):
        """–ö—Ä–æ–∫ 6: –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ"""
        print("=" * 70)
        print("–ö–†–û–ö 6: –û—Ü—ñ–Ω–∫–∞ –º–æ–¥–µ–ª—ñ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö")
        print("=" * 70)

        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
        y_pred = self.model.predict(self.X_test)
        y_pred_proba = self.model.predict_proba(self.X_test)[:, 1]

        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(self.y_test, y_pred)
        f1 = f1_score(self.y_test, y_pred)
        roc_auc = roc_auc_score(self.y_test, y_pred_proba)

        print(f"üìà –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ñ–π –≤–∏–±—ñ—Ä—Ü—ñ:")
        print(f"   Accuracy:  {accuracy:.4f}")
        print(f"   F1-Score:  {f1:.4f}")
        print(f"   ROC-AUC:   {roc_auc:.4f}\n")

        # –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç
        print("üìã –î–µ—Ç–∞–ª—å–Ω–∏–π –∑–≤—ñ—Ç –∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—ó:")
        print(classification_report(self.y_test, y_pred,
                                    target_names=['Active (0)', 'Churned (1)']))

        # Confusion Matrix
        cm = confusion_matrix(self.y_test, y_pred)
        print("üî≤ Confusion Matrix:")
        print(f"   True Negatives:  {cm[0, 0]:>5}")
        print(f"   False Positives: {cm[0, 1]:>5}")
        print(f"   False Negatives: {cm[1, 0]:>5}")
        print(f"   True Positives:  {cm[1, 1]:>5}\n")

        return y_pred, y_pred_proba, accuracy, f1, roc_auc, cm

    def plot_results(self, y_pred, y_pred_proba, cm):
        """–ö—Ä–æ–∫ 7: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""
        print("=" * 70)
        print("–ö–†–û–ö 7: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤")
        print("=" * 70)

        fig = plt.figure(figsize=(18, 12))
        gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)

        # 1. Confusion Matrix
        ax1 = fig.add_subplot(gs[0, 0])
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, ax=ax1)
        ax1.set_title('Confusion Matrix', fontweight='bold', fontsize=12)
        ax1.set_ylabel('True Label')
        ax1.set_xlabel('Predicted Label')
        ax1.set_xticklabels(['Active', 'Churned'])
        ax1.set_yticklabels(['Active', 'Churned'])

        # 2. ROC Curve
        ax2 = fig.add_subplot(gs[0, 1])
        fpr, tpr, _ = roc_curve(self.y_test, y_pred_proba)
        roc_auc = roc_auc_score(self.y_test, y_pred_proba)
        ax2.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC AUC = {roc_auc:.4f}')
        ax2.plot([0, 1], [0, 1], 'r--', linewidth=1, label='Random Classifier')
        ax2.set_xlabel('False Positive Rate')
        ax2.set_ylabel('True Positive Rate')
        ax2.set_title('ROC Curve', fontweight='bold', fontsize=12)
        ax2.legend(loc='lower right')
        ax2.grid(True, alpha=0.3)

        # 3. Precision-Recall Curve
        ax3 = fig.add_subplot(gs[0, 2])
        precision, recall, _ = precision_recall_curve(self.y_test, y_pred_proba)
        ax3.plot(recall, precision, 'g-', linewidth=2)
        ax3.set_xlabel('Recall')
        ax3.set_ylabel('Precision')
        ax3.set_title('Precision-Recall Curve', fontweight='bold', fontsize=12)
        ax3.grid(True, alpha=0.3)

        # 4. Feature Correlation with Churn (—Ç–æ–ø-15)
        ax4 = fig.add_subplot(gs[1, :])

        # –û–±—á–∏—Å–ª–µ–Ω–Ω—è –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –º—ñ–∂ –æ–∑–Ω–∞–∫–∞–º–∏ —Ç–∞ is_churned
        feature_correlations = []
        for feature in self.X_train.columns:
            # –û–±'—î–¥–Ω—É—î–º–æ train —Ç–∞ test –¥–ª—è –ø–æ–≤–Ω–æ—ó –∫–∞—Ä—Ç–∏–Ω–∏
            all_X = pd.concat([self.X_train, self.X_test])
            all_y = pd.concat([self.y_train, self.y_test])

            correlation = all_X[feature].corr(all_y)
            feature_correlations.append({
                'feature': feature,
                'correlation': correlation,
                'coefficient': self.model.coef_[0][list(self.X_train.columns).index(feature)]
            })

        # –°–æ—Ä—Ç—É–≤–∞–Ω–Ω—è –∑–∞ –∞–±—Å–æ–ª—é—Ç–Ω–æ—é –∫–æ—Ä–µ–ª—è—Ü—ñ—î—é
        corr_df = pd.DataFrame(feature_correlations).sort_values('correlation', key=abs, ascending=False).head(15)

        # –Ü–Ω–≤–µ—Ä—Ç—É—î–º–æ –ø–æ—Ä—è–¥–æ–∫ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è (–Ω–∞–π—Å–∏–ª—å–Ω—ñ—à–∏–π –≤–≥–æ—Ä—ñ)
        corr_df = corr_df.iloc[::-1]

        # –ö–æ–ª—å–æ—Ä–∏: —á–µ—Ä–≤–æ–Ω–∏–π = –ø—ñ–¥–≤–∏—â—É—î churn (–Ω–µ–≥–∞—Ç–∏–≤), –∑–µ–ª–µ–Ω–∏–π = –∑–Ω–∏–∂—É—î churn (–ø–æ–∑–∏—Ç–∏–≤)
        colors = ['#d32f2f' if x > 0 else '#388e3c' for x in corr_df['correlation']]

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –±–∞—Ä—ñ–≤
        bars = ax4.barh(range(len(corr_df)), corr_df['correlation'], color=colors, alpha=0.75, edgecolor='black',
                        linewidth=0.8)

        # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –æ—Å–µ–π
        ax4.set_yticks(range(len(corr_df)))
        ax4.set_yticklabels(corr_df['feature'], fontsize=10)
        ax4.set_xlabel('Correlation with Churn (negative ‚Üê  |  ‚Üí  positive)', fontsize=11)
        ax4.set_title('Top 15 Features: Correlation with Churn Risk', fontweight='bold', fontsize=13)

        # –í–µ—Ä—Ç–∏–∫–∞–ª—å–Ω–∞ –ª—ñ–Ω—ñ—è –Ω–∞ –Ω—É–ª—ñ (–≤–∏—Ä—ñ–≤–Ω—è–Ω–∞ –ø–æ —Ü–µ–Ω—Ç—Ä—É –±–∞—Ä—ñ–≤)
        ax4.axvline(x=0, color='#424242', linestyle='-', linewidth=1.5, zorder=0)

        # –ì–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–Ω—ñ –ª—ñ–Ω—ñ—ó —Å—ñ—Ç–∫–∏ –Ω–∞ —Ä—ñ–≤–Ω—ñ –∫–æ–∂–Ω–æ–≥–æ –±–∞—Ä—É (–Ω–µ —Å–µ—Ä–µ–¥–∏–Ω–∏)
        for i in range(len(corr_df) + 1):
            ax4.axhline(y=i - 0.5, color='gray', linestyle='-', linewidth=0.5, alpha=0.3, zorder=0)

        # –î–æ–¥–∞–≤–∞–Ω–Ω—è –∑–Ω–∞—á–µ–Ω—å –∫–æ—Ä–µ–ª—è—Ü—ñ—ó –Ω–∞ –±–∞—Ä–∞—Ö
        for i, (idx, row) in enumerate(corr_df.iterrows()):
            value = row['correlation']
            x_pos = value + (0.01 if value > 0 else -0.01)
            ha = 'left' if value > 0 else 'right'
            ax4.text(x_pos, i, f'{value:.3f}', va='center', ha=ha, fontsize=9, fontweight='bold')

        # –õ–µ–≥–µ–Ω–¥–∞
        from matplotlib.patches import Patch
        legend_elements = [
            Patch(facecolor='#d32f2f', alpha=0.75, edgecolor='black', label='Increases Churn Risk'),
            Patch(facecolor='#388e3c', alpha=0.75, edgecolor='black', label='Decreases Churn Risk')
        ]
        ax4.legend(handles=legend_elements, loc='lower right', fontsize=10)

        # –í—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—è –º–µ–∂—ñ –æ—Å—ñ X —Å–∏–º–µ—Ç—Ä–∏—á–Ω–æ
        max_abs_corr = corr_df['correlation'].abs().max()
        ax4.set_xlim(-max_abs_corr * 1.15, max_abs_corr * 1.15)

        # 5. Predicted Probability Distribution
        ax5 = fig.add_subplot(gs[2, 0])
        ax5.hist(y_pred_proba[self.y_test == 0], bins=30, alpha=0.6,
                 label='Active (0)', color='blue', edgecolor='black')
        ax5.hist(y_pred_proba[self.y_test == 1], bins=30, alpha=0.6,
                 label='Churned (1)', color='red', edgecolor='black')
        ax5.set_xlabel('Predicted Probability')
        ax5.set_ylabel('Frequency')
        ax5.set_title('Predicted Probability Distribution', fontweight='bold', fontsize=12)
        ax5.legend()
        ax5.grid(True, alpha=0.3)

        # 6. Class Distribution
        ax6 = fig.add_subplot(gs[2, 1])
        class_counts = [sum(self.y_test == 0), sum(self.y_test == 1)]
        ax6.bar(['Active (0)', 'Churned (1)'], class_counts,
                color=['blue', 'red'], alpha=0.7, edgecolor='black')
        ax6.set_ylabel('Count')
        ax6.set_title('Test Set Class Distribution', fontweight='bold', fontsize=12)
        ax6.grid(True, alpha=0.3, axis='y')
        for i, v in enumerate(class_counts):
            ax6.text(i, v + 5, str(v), ha='center', fontweight='bold')

        # 7. Prediction Distribution
        ax7 = fig.add_subplot(gs[2, 2])
        pred_counts = [sum(y_pred == 0), sum(y_pred == 1)]
        ax7.bar(['Active (0)', 'Churned (1)'], pred_counts,
                color=['blue', 'red'], alpha=0.7, edgecolor='black')
        ax7.set_ylabel('Count')
        ax7.set_title('Predicted Class Distribution', fontweight='bold', fontsize=12)
        ax7.grid(True, alpha=0.3, axis='y')
        for i, v in enumerate(pred_counts):
            ax7.text(i, v + 5, str(v), ha='center', fontweight='bold')

        plt.savefig('churn_prediction_results.png', dpi=300, bbox_inches='tight')
        print("‚úì –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É 'churn_prediction_results.png'")
        plt.show()
        print()

    def plot_feature_correlation_matrix(self, save_path="feature_correlation_matrix.png", figsize=(16, 14)):
        """
        –ë—É–¥—É—î heatmap –∫–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ —Ñ—ñ—á (–≤—Å—ñ —á–∏—Å–ª–æ–≤—ñ —Ñ—ñ—á—ñ –ø—ñ—Å–ª—è –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å–∏–Ω–≥—É)
        —ñ –∑–±–µ—Ä—ñ–≥–∞—î —É –æ–∫—Ä–µ–º–∏–π —Ñ–∞–π–ª.
        """

        if self.X_train is None or self.X_test is None:
            raise ValueError("–î–∞–Ω—ñ –Ω–µ –ø—ñ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ñ. –°–ø–æ—á–∞—Ç–∫—É –≤–∏–∫–ª–∏–∫–∞–π split_and_scale().")

        # –û–±‚Äô—î–¥–Ω—É—î–º–æ train + test –¥–ª—è –ø–æ–≤–Ω—ñ—à–æ—ó –º–∞—Ç—Ä–∏—Ü—ñ
        all_X = pd.concat([self.X_train, self.X_test])

        # –û–±—á–∏—Å–ª—é—î–º–æ –∫–æ—Ä–µ–ª—è—Ü—ñ—é
        corr = all_X.corr()

        # –ú–∞–ª—é—î–º–æ
        plt.figure(figsize=figsize)
        sns.heatmap(
            corr,
            cmap="coolwarm",
            annot=False,
            cbar=True,
            square=True,
            linewidths=0.5,
            linecolor="gray"
        )
        plt.title("Feature Correlation Matrix", fontsize=16, fontweight="bold")
        plt.xticks(rotation=90)
        plt.yticks(rotation=0)
        plt.tight_layout()

        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ
        plt.savefig(save_path, dpi=300)
        plt.close()

        print(f"‚úì –ö–æ—Ä–µ–ª—è—Ü—ñ–π–Ω–∞ –º–∞—Ç—Ä–∏—Ü—è —Ñ—ñ—á –∑–±–µ—Ä–µ–∂–µ–Ω–∞ —É —Ñ–∞–π–ª ¬´{save_path}¬ª")

    def save_model(self, model_path='churn_model.pkl'):
        """–ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ"""
        import pickle

        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.X_train.columns.tolist(),
            'feature_importance': self.feature_importance
        }

        with open(model_path, 'wb') as f:
            pickle.dump(model_data, f)

        print(f"‚úì –ú–æ–¥–µ–ª—å –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É '{model_path}'")

    def predict_new_user(self, user_data):
        """–ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞"""
        # –ü–µ—Ä–µ–∫–æ–Ω–∞—Ç–∏—Å—è, —â–æ –≤—Å—ñ –æ–∑–Ω–∞–∫–∏ –ø—Ä–∏—Å—É—Ç–Ω—ñ
        user_df = pd.DataFrame([user_data])
        user_df = user_df[self.X_train.columns]

        # –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        user_scaled = self.scaler.transform(user_df)

        # –ü–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è
        prediction = self.model.predict(user_scaled)[0]
        probability = self.model.predict_proba(user_scaled)[0]

        return {
            'prediction': 'Churned' if prediction == 1 else 'Active',
            'churn_probability': probability[1],
            'active_probability': probability[0]
        }


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è"""

    print("\n" + "=" * 70)
    print("üéØ –ú–û–î–ï–õ–¨ –ü–ï–†–ï–î–ë–ê–ß–ï–ù–ù–Ø CHURN –ö–û–†–ò–°–¢–£–í–ê–ß–Ü–í")
    print("=" * 70 + "\n")

    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    filepath = 'user_features.csv'  # –ê–±–æ 'cleaned.csv' –¥–ª—è –æ–±—Ä–æ–±–ª–µ–Ω–æ–≥–æ
    use_cleaned = False  # True —è–∫—â–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç–µ cleaned.csv
    use_categories = True  # False –¥–ª—è multi-hot encoding –∫–æ–∂–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–Ω—è

    print(f"‚öôÔ∏è  –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è:")
    print(f"   –§–∞–π–ª: {filepath}")
    print(f"   –†–µ–∂–∏–º –∫–∞—Ç–µ–≥–æ—Ä—ñ–π: {'–ì—Ä—É–ø—É–≤–∞–Ω–Ω—è' if use_categories else 'Multi-hot encoding'}")
    print()

    # –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è
    model = ChurnPredictionModel(filepath, use_cleaned=use_cleaned, use_categories=use_categories)

    # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –µ—Ç–∞–ø—ñ–≤
    model.load_and_prepare_data()
    X, y = model.feature_engineering()
    model.split_and_scale(X, y, test_size=0.2)
    model.train_model(max_iter=1000, solver='lbfgs', class_weight='balanced')
    cv_scores = model.cross_validate(cv=5)
    y_pred, y_pred_proba, accuracy, f1, roc_auc, cm = model.evaluate_model()
    model.plot_results(y_pred, y_pred_proba, cm)
    model.plot_feature_correlation_matrix()

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
    model.save_model('churn_model.pkl')

    # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è feature importance
    model.feature_importance.to_csv('feature_importance.csv', index=False)
    print("‚úì Feature importance –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É 'feature_importance.csv'\n")

    print("=" * 70)
    print("‚úÖ –ú–û–î–ï–õ–Æ–í–ê–ù–ù–Ø –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–Ü–®–ù–û!")
    print("=" * 70)
    print("\n–§–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ:")
    print("  ‚Ä¢ churn_prediction_results.png - –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó (7 –≥—Ä–∞—Ñ—ñ–∫—ñ–≤)")
    print("  ‚Ä¢ churn_model.pkl - –Ω–∞–≤—á–µ–Ω–∞ –º–æ–¥–µ–ª—å")
    print("  ‚Ä¢ feature_importance.csv - –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å –æ–∑–Ω–∞–∫")
    print("\n–ú–µ—Ç—Ä–∏–∫–∏:")
    print(f"  ‚Ä¢ Accuracy:  {accuracy:.4f}")
    print(f"  ‚Ä¢ F1-Score:  {f1:.4f}")
    print(f"  ‚Ä¢ ROC-AUC:   {roc_auc:.4f}")

    # –ü—Ä–∏–∫–ª–∞–¥ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è –¥–ª—è –Ω–æ–≤–æ–≥–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    print("\n" + "=" * 70)
    print("üìù –ü–†–ò–ö–õ–ê–î –ü–ï–†–ï–î–ë–ê–ß–ï–ù–ù–Ø –î–õ–Ø –ù–û–í–û–ì–û –ö–û–†–ò–°–¢–£–í–ê–ß–ê")
    print("=" * 70)

    example_user = {col: X.iloc[0][col] for col in X.columns}
    result = model.predict_new_user(example_user)

    print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç –ø–µ—Ä–µ–¥–±–∞—á–µ–Ω–Ω—è:")
    print(f"  ‚Ä¢ –°—Ç–∞—Ç—É—Å: {result['prediction']}")
    print(f"  ‚Ä¢ –ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å churn: {result['churn_probability']:.2%}")
    print(f"  ‚Ä¢ –ô–º–æ–≤—ñ—Ä–Ω—ñ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ: {result['active_probability']:.2%}")


if __name__ == "__main__":
    main()