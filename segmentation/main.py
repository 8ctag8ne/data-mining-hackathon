import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import RobustScaler
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score
from sklearn.cluster import KMeans
import umap
import hdbscan
import warnings

warnings.filterwarnings('ignore')

# –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è –¥–ª—è –∫—Ä–∞—â–æ—ó –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
plt.style.use('seaborn-v0_8-darkgrid')
sns.set_palette("husl")


class UserSegmentation:
    def __init__(self, filepath):
        """–Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—É –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤"""
        self.filepath = filepath
        self.df_original = None
        self.df_cleaned = None
        self.df_features = None
        self.df_normalized = None
        self.umap_embedding = None
        self.hdbscan_model = None
        self.kmeans_model = None
        self.optimal_k = None
        self.scaler = RobustScaler()
        self.n_clusters_found = None

    def load_and_clean_data(self):
        """–ö—Ä–æ–∫ 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ—á–∏—â–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É"""
        print("=" * 60)
        print("–ö–†–û–ö 1: –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–∞ –æ—á–∏—â–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É")
        print("=" * 60)

        # –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö
        self.df_original = pd.read_csv(self.filepath)
        print(f"‚úì –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(self.df_original)} –∑–∞–ø–∏—Å—ñ–≤")
        print(f"‚úì –ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ–ª–æ–Ω–æ–∫: {len(self.df_original.columns)}")

        self.df_cleaned = self.df_original.copy()

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤
        duplicates = self.df_cleaned.duplicated().sum()
        self.df_cleaned = self.df_cleaned.drop_duplicates()
        print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –¥—É–±–ª—ñ–∫–∞—Ç—ñ–≤: {duplicates}")

        # –û–±—Ä–æ–±–∫–∞ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å
        missing_before = self.df_cleaned.isnull().sum().sum()
        numeric_cols = self.df_cleaned.select_dtypes(include=[np.number]).columns

        # –°–ø–µ—Ü—ñ–∞–ª—å–Ω–∞ –æ–±—Ä–æ–±–∫–∞ time_to_first_message_sec
        no_messages = self.df_cleaned['time_to_first_message_sec'].isnull()
        max_time = self.df_cleaned['time_to_first_message_sec'].max()
        self.df_cleaned.loc[no_messages, 'time_to_first_message_sec'] = max_time * 2

        for col in numeric_cols:
            if self.df_cleaned[col].isnull().sum() > 0:
                self.df_cleaned[col].fillna(self.df_cleaned[col].median(), inplace=True)

        print(f"‚úì –ó–∞–ø–æ–≤–Ω–µ–Ω–æ –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å: {missing_before}")

        # -------------------------------------------------------------------
        # üü©  MULTI-HOT ENCODING –ó –ê–ì–†–ï–ì–ê–¶–Ü–Ñ–Æ –£ –ö–ê–¢–ï–ì–û–†–Ü–á
        # -------------------------------------------------------------------
        print("‚úì –í–∏–∫–æ–Ω—É—î—Ç—å—Å—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü—ñ—è goals / interests / assistance...")

        def split_values(s):
            if pd.isna(s) or s == "":
                return []
            return [x.strip().lower() for x in str(s).split(",") if x.strip()]

        # –Ω–æ—Ä–º–∞–ª—ñ–∑—É—î–º–æ —Å–ø–∏—Å–∫–∏
        self.df_cleaned["goals_list"] = self.df_cleaned["user_goals"].apply(split_values)
        self.df_cleaned["interests_list"] = self.df_cleaned["user_interests"].apply(split_values)
        self.df_cleaned["assistance_list"] = self.df_cleaned["user_assistance"].apply(split_values)

        # -----------------------------------------
        # –ö–ê–¢–ï–ì–û–†–Ü–á –î–õ–Ø –ê–ì–†–ï–ì–ê–¶–Ü–á
        # -----------------------------------------

        GOALS_CATEGORIES = {
            "goals_creative": ["content-creation", "essay-writing", "cooking"],
            "goals_professional": ["coding-assistance", "business-purposes"],
            "goals_personal_dev": ["education", "mental-health"],
            "goals_social_entertain": ["social-media", "entertainment"]
        }

        ASSISTANCE_CATEGORIES = {
            "assist_detailed": ["step-by-step", "detailed-explanation", "answer-with-explanation"],
            "assist_concise": ["direct-answer", "simplified-explanation"]
        }

        INTEREST_CATEGORIES = {
            "interest_creative_arts": ["arts", "music", "writing", "dance", "movies"],
            "interest_practical": ["diy", "cooking", "gardening"],
            "interest_outdoor": ["sports", "outdoor", "travel"],
            "interest_intellectual": ["reading", "learning", "history"],
            "interest_business_tech": ["business", "technology"],
            "interest_lifestyle": ["fashion", "family", "animals"]
        }

        # -----------------------------------------
        # –§–£–ù–ö–¶–Ü–Ø –î–õ–Ø –°–¢–í–û–†–ï–ù–ù–Ø –ë–Ü–ù–ê–†–ù–ò–• –û–ó–ù–ê–ö
        # -----------------------------------------

        def assign_categories(df, source_col, categories):
            for new_col, group_values in categories.items():
                df[new_col] = df[source_col].apply(
                    lambda lst: int(any(item in lst for item in group_values))
                )
            return df

        # –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è
        self.df_cleaned = assign_categories(self.df_cleaned, "goals_list", GOALS_CATEGORIES)
        self.df_cleaned = assign_categories(self.df_cleaned, "assistance_list", ASSISTANCE_CATEGORIES)
        self.df_cleaned = assign_categories(self.df_cleaned, "interests_list", INTEREST_CATEGORIES)

        print(f"‚úì –°—Ç–≤–æ—Ä–µ–Ω–æ {len(GOALS_CATEGORIES)} –∫–∞—Ç–µ–≥–æ—Ä—ñ–π goals")
        print(f"‚úì –°—Ç–≤–æ—Ä–µ–Ω–æ {len(ASSISTANCE_CATEGORIES)} –∫–∞—Ç–µ–≥–æ—Ä—ñ–π assistance")
        print(f"‚úì –°—Ç–≤–æ—Ä–µ–Ω–æ {len(INTEREST_CATEGORIES)} –∫–∞—Ç–µ–≥–æ—Ä—ñ–π interests")

        # –≤–∏–¥–∞–ª—è—î–º–æ —Å–∏—Ä—ñ –∫–æ–ª–æ–Ω–∫–∏
        self.df_cleaned.drop(columns=[
            "goals_list", "interests_list", "assistance_list",
            "user_goals", "user_interests", "user_assistance"
        ], inplace=True)

        # -------------------------------------------------------------------

        len_before = len(self.df_cleaned)

        print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –≤–∏–∫–∏–¥—ñ–≤ (outliers): {len_before - len(self.df_cleaned)}")
        print(f"‚úì –ó–∞–ª–∏—à–∏–ª–æ—Å—å –∑–∞–ø–∏—Å—ñ–≤ –ø—ñ—Å–ª—è –æ—á–∏—â–µ–Ω–Ω—è: {len(self.df_cleaned)}\n")

    def feature_selection(self):
        """–ö—Ä–æ–∫ 2: –í—ñ–¥–±—ñ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –æ–∑–Ω–∞–∫"""
        print("=" * 60)
        print("–ö–†–û–ö 2: –í—ñ–¥–±—ñ—Ä —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –æ–∑–Ω–∞–∫")
        print("=" * 60)

        self.df_features = self.df_cleaned.copy()

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä—ñ–≤ —Ç–∞ –Ω–µ—Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
        columns_to_remove = ['user_id', 'model_changes']
        self.df_features = self.df_features.drop(columns=columns_to_remove, errors='ignore')
        print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ —ñ–¥–µ–Ω—Ç–∏—Ñ—ñ–∫–∞—Ç–æ—Ä–∏: {columns_to_remove}")

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –∫–æ–ª–æ–Ω–æ–∫ –∑ –Ω–∏–∑—å–∫–æ—é –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—é (std < 0.01)
        numeric_cols = self.df_features.select_dtypes(include=[np.number]).columns
        low_variance_cols = []

        for col in numeric_cols:
            if self.df_features[col].std() < 0.01:
                low_variance_cols.append(col)

        if low_variance_cols:
            self.df_features = self.df_features.drop(columns=low_variance_cols)
            print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –∫–æ–ª–æ–Ω–æ–∫ –∑ –Ω–∏–∑—å–∫–æ—é –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—é: {low_variance_cols}")
        else:
            print("‚úì –ö–æ–ª–æ–Ω–æ–∫ –∑ –Ω–∏–∑—å–∫–æ—é –≤–∞—Ä—ñ–∞—Ç–∏–≤–Ω—ñ—Å—Ç—é –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

        # –í–∏–¥–∞–ª–µ–Ω–Ω—è –≤–∏—Å–æ–∫–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –æ–∑–Ω–∞–∫
        correlation_matrix = self.df_features.corr().abs()
        upper_triangle = correlation_matrix.where(
            np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)
        )

        high_corr_cols = []
        for column in upper_triangle.columns:
            if any(upper_triangle[column] > 0.9):
                high_corr_cols.append(column)

        if high_corr_cols:
            self.df_features = self.df_features.drop(columns=high_corr_cols)
            print(f"‚úì –í–∏–¥–∞–ª–µ–Ω–æ –≤–∏—Å–æ–∫–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ (r > 0.9): {high_corr_cols}")
        else:
            print("‚úì –í–∏—Å–æ–∫–æ –∫–æ—Ä–µ–ª—å–æ–≤–∞–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ")

        print(f"‚úì –ó–∞–ª–∏—à–∏–ª–æ—Å—å –æ–∑–Ω–∞–∫ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó: {len(self.df_features.columns)}")
        print(f"  –û–∑–Ω–∞–∫–∏: {list(self.df_features.columns)}\n")

    def normalize_data(self):
        """–ö—Ä–æ–∫ 3: –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö"""
        print("=" * 60)
        print("–ö–†–û–ö 3: –ù–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è –¥–∞–Ω–∏—Ö")
        print("=" * 60)

        self.df_normalized = pd.DataFrame(
            self.scaler.fit_transform(self.df_features),
            columns=self.df_features.columns,
            index=self.df_features.index
        )

        print("‚úì –î–∞–Ω—ñ –Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–æ –∑–∞ –¥–æ–ø–æ–º–æ–≥–æ—é MinMaxScaler")
        print(f"  –°–µ—Ä–µ–¥–Ω—î –∑–Ω–∞—á–µ–Ω–Ω—è –ø—ñ—Å–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó: {self.df_normalized.mean().mean():.4f}")
        print(f"  –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–µ –≤—ñ–¥—Ö–∏–ª–µ–Ω–Ω—è –ø—ñ—Å–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó: {self.df_normalized.std().mean():.4f}\n")

    def find_optimal_clusters(self, max_k=10):
        """–ö—Ä–æ–∫ 4: UMAP –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ + HDBSCAN + –∞–Ω–∞–ª—ñ–∑ K-Means"""
        print("=" * 60)
        print("–ö–†–û–ö 4: UMAP + HDBSCAN + –æ–ø—Ç–∏–º—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤")
        print("=" * 60)

        # UMAP –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ
        print("–í–∏–∫–æ–Ω—É—î—Ç—å—Å—è UMAP –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ...")
        umap_model = umap.UMAP(
            n_neighbors=15,
            min_dist=0.1,
            n_components=5,  # –ë—ñ–ª—å—à–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç –¥–ª—è –∫—Ä–∞—â–æ–≥–æ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏
            metric='euclidean',
            random_state=42
        )
        self.umap_embedding = umap_model.fit_transform(self.df_normalized)
        print(f"‚úì UMAP –∑–∞–≤–µ—Ä—à–µ–Ω–æ: {self.df_normalized.shape[1]} –æ–∑–Ω–∞–∫ ‚Üí {self.umap_embedding.shape[1]} –∫–æ–º–ø–æ–Ω–µ–Ω—Ç\n")

        # HDBSCAN –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤
        print("–í–∏–∫–æ–Ω—É—î—Ç—å—Å—è HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è...")
        self.hdbscan_model = hdbscan.HDBSCAN(
            min_cluster_size=30,
            min_samples=10,
            metric='euclidean',
            cluster_selection_epsilon=0.0,
            cluster_selection_method='eom'
        )
        hdbscan_labels = self.hdbscan_model.fit_predict(self.umap_embedding)

        # –ê–Ω–∞–ª—ñ–∑ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤ HDBSCAN
        n_clusters_hdbscan = len(set(hdbscan_labels)) - (1 if -1 in hdbscan_labels else 0)
        n_noise = list(hdbscan_labels).count(-1)

        print(f"‚úì HDBSCAN –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
        print(f"  –ó–Ω–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters_hdbscan}")
        print(f"  –®—É–º–æ–≤–∏—Ö —Ç–æ—á–æ–∫ (outliers): {n_noise} ({n_noise / len(hdbscan_labels) * 100:.1f}%)")

        if n_clusters_hdbscan > 0:
            valid_labels = hdbscan_labels[hdbscan_labels != -1]
            valid_data = self.umap_embedding[hdbscan_labels != -1]
            if len(valid_labels) > 0:
                hdbscan_silhouette = silhouette_score(valid_data, valid_labels)
                print(f"  Silhouette Score (–±–µ–∑ —à—É–º—É): {hdbscan_silhouette:.4f}\n")

        # K-Means –Ω–∞ UMAP embedding –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è
        print("–û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ K-Means –Ω–∞ UMAP embedding...")
        inertias = []
        silhouette_scores = []
        davies_bouldin_scores = []
        calinski_harabasz_scores = []
        K_range = range(2, max_k + 1)

        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(self.umap_embedding)

            inertias.append(kmeans.inertia_)
            silhouette_avg = silhouette_score(self.umap_embedding, labels)
            silhouette_scores.append(silhouette_avg)

            db_score = davies_bouldin_score(self.umap_embedding, labels)
            davies_bouldin_scores.append(db_score)

            ch_score = calinski_harabasz_score(self.umap_embedding, labels)
            calinski_harabasz_scores.append(ch_score)

            print(f"  k={k}: Silhouette={silhouette_avg:.4f}, Inertia={kmeans.inertia_:.2f}")

        # –í–∏–∑–Ω–∞—á–µ–Ω–Ω—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ k
        self.optimal_k = K_range[np.argmax(silhouette_scores)]
        print(f"\n‚úì –û–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (K-Means –∑–∞ Silhouette): {self.optimal_k}")
        print(f"  –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π Silhouette Score: {max(silhouette_scores):.4f}")
        print(f"  HDBSCAN —Ä–µ–∫–æ–º–µ–Ω–¥—É—î: {n_clusters_hdbscan} –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤\n")

        return K_range, inertias, silhouette_scores, davies_bouldin_scores, calinski_harabasz_scores, n_clusters_hdbscan

    def create_clusters(self, n_clusters=None, method='kmeans'):
        """–ö—Ä–æ–∫ 5: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è —Ñ—ñ–Ω–∞–ª—å–Ω–∏—Ö –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤"""
        print("=" * 60)
        print("–ö–†–û–ö 5: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤")
        print("=" * 60)

        if method == 'hdbscan':
            print("–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è...")
            labels = self.hdbscan_model.labels_
            n_clusters_found = len(set(labels)) - (1 if -1 in labels else 0)
            n_noise = list(labels).count(-1)

            # –ü—Ä–∏—Å–≤–æ—é—î–º–æ —à—É–º–æ–≤—ñ —Ç–æ—á–∫–∏ –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
            if n_noise > 0:
                print(f"‚ö† –ó–Ω–∞–π–¥–µ–Ω–æ {n_noise} —à—É–º–æ–≤–∏—Ö —Ç–æ—á–æ–∫, –ø—Ä–∏—Å–≤–æ—é—é—Ç—å—Å—è –¥–æ –Ω–∞–π–±–ª–∏–∂—á–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞...")
                noise_mask = labels == -1
                if noise_mask.sum() > 0:
                    from sklearn.neighbors import NearestNeighbors
                    nn = NearestNeighbors(n_neighbors=1)
                    nn.fit(self.umap_embedding[~noise_mask])
                    _, indices = nn.kneighbors(self.umap_embedding[noise_mask])
                    labels[noise_mask] = labels[~noise_mask][indices.flatten()]

            self.df_features['cluster'] = labels
            self.n_clusters_found = n_clusters_found

            print(f"‚úì HDBSCAN –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
            print(f"  –ó–Ω–∞–π–¥–µ–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters_found}")

        else:  # kmeans
            if n_clusters is None:
                n_clusters = self.optimal_k
                print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters}")
            else:
                print(f"–í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –∑–∞–¥–∞–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {n_clusters}")

            self.kmeans_model = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            labels = self.kmeans_model.fit_predict(self.umap_embedding)
            self.df_features['cluster'] = labels
            self.n_clusters_found = n_clusters

            print(f"‚úì K-Means –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

        # –ú–µ—Ç—Ä–∏–∫–∏
        silhouette_avg = silhouette_score(self.umap_embedding, labels)
        db_score = davies_bouldin_score(self.umap_embedding, labels)
        ch_score = calinski_harabasz_score(self.umap_embedding, labels)

        print(f"  Silhouette Score: {silhouette_avg:.4f}")
        print(f"  Davies-Bouldin Index: {db_score:.4f} (–Ω–∏–∂—á–µ = –∫—Ä–∞—â–µ)")
        print(f"  Calinski-Harabasz Score: {ch_score:.2f} (–≤–∏—â–µ = –∫—Ä–∞—â–µ)\n")

    def get_centroids(self):
        """–ö—Ä–æ–∫ 6: –û—Ç—Ä–∏–º–∞–Ω–Ω—è —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤"""
        print("=" * 60)
        print("–ö–†–û–ö 6: –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤")
        print("=" * 60)

        # –û–±—á–∏—Å–ª—é—î–º–æ —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∏ —è–∫ —Å–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –≤ –æ—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–æ–º—É –ø—Ä–æ—Å—Ç–æ—Ä—ñ
        centroids_list = []
        for cluster in sorted(self.df_features['cluster'].unique()):
            cluster_mask = self.df_features['cluster'] == cluster
            cluster_data = self.df_features[cluster_mask].drop(columns=['cluster'])
            centroid = cluster_data.mean()
            centroids_list.append(centroid)

        centroids_df = pd.DataFrame(centroids_list)
        centroids_df.index = [f"–ö–ª–∞—Å—Ç–µ—Ä {i}" for i in range(len(centroids_df))]

        # –î–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—è
        centroids_denorm = self.scaler.inverse_transform(centroids_df)
        centroids_df = pd.DataFrame(
            centroids_denorm,
            columns=centroids_df.columns,
            index=centroids_df.index
        )

        print("–¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (–¥–µ–Ω–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è):")
        print(centroids_df.round(2))
        print()

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ —É CSV
        centroids_df.to_csv('cluster_centroids.csv')
        print("‚úì –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É 'cluster_centroids.csv'")

        # –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ —É TXT
        with open('cluster_centroids.txt', 'w', encoding='utf-8') as f:
            f.write("=" * 80 + "\n")
            f.write("–¶–ï–ù–¢–†–û–á–î–ò –ö–õ–ê–°–¢–ï–†–Ü–í –ö–û–†–ò–°–¢–£–í–ê–ß–Ü–í (UMAP + HDBSCAN/K-Means)\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"–î–∞—Ç–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤: {len(centroids_df)}\n")
            f.write(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –æ–∑–Ω–∞–∫: {len(centroids_df.columns)}\n\n")

            for idx, row in centroids_df.iterrows():
                f.write(f"{'‚îÄ' * 80}\n")
                f.write(f"{idx}\n")
                f.write(f"{'‚îÄ' * 80}\n")
                for col, val in row.items():
                    f.write(f"  {col:.<40} {val:>12.4f}\n")
                f.write("\n")

        print("‚úì –¶–µ–Ω—Ç—Ä–æ—ó–¥–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É 'cluster_centroids.txt'\n")

        return centroids_df

    def plot_metrics(self, K_range, inertias, silhouette_scores, davies_bouldin_scores, calinski_harabasz_scores):
        """–ö—Ä–æ–∫ 7: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ—Ç—Ä–∏–∫"""
        print("=" * 60)
        print("–ö–†–û–ö 7: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –º–µ—Ç—Ä–∏–∫")
        print("=" * 60)

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))

        # Elbow Method
        axes[0, 0].plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)
        axes[0, 0].set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (k)', fontsize=12)
        axes[0, 0].set_ylabel('Inertia (Within-Cluster Sum of Squares)', fontsize=12)
        axes[0, 0].set_title('Elbow Method (K-Means on UMAP)', fontsize=14, fontweight='bold')
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].set_xticks(K_range)

        # Silhouette Score
        axes[0, 1].plot(K_range, silhouette_scores, 'ro-', linewidth=2, markersize=8)
        axes[0, 1].axvline(x=self.optimal_k, color='green', linestyle='--',
                           label=f'–û–ø—Ç–∏–º–∞–ª—å–Ω–µ k={self.optimal_k}', linewidth=2)
        axes[0, 1].set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (k)', fontsize=12)
        axes[0, 1].set_ylabel('Silhouette Score', fontsize=12)
        axes[0, 1].set_title('Silhouette Score Method', fontsize=14, fontweight='bold')
        axes[0, 1].grid(True, alpha=0.3)
        axes[0, 1].legend()
        axes[0, 1].set_xticks(K_range)

        # Davies-Bouldin Index
        axes[1, 0].plot(K_range, davies_bouldin_scores, 'go-', linewidth=2, markersize=8)
        axes[1, 0].set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (k)', fontsize=12)
        axes[1, 0].set_ylabel('Davies-Bouldin Index (Lower is Better)', fontsize=12)
        axes[1, 0].set_title('Davies-Bouldin Index', fontsize=14, fontweight='bold')
        axes[1, 0].grid(True, alpha=0.3)
        axes[1, 0].set_xticks(K_range)

        # Calinski-Harabasz Score
        axes[1, 1].plot(K_range, calinski_harabasz_scores, 'mo-', linewidth=2, markersize=8)
        axes[1, 1].set_xlabel('–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (k)', fontsize=12)
        axes[1, 1].set_ylabel('Calinski-Harabasz Score (Higher is Better)', fontsize=12)
        axes[1, 1].set_title('Calinski-Harabasz Score', fontsize=14, fontweight='bold')
        axes[1, 1].grid(True, alpha=0.3)
        axes[1, 1].set_xticks(K_range)

        plt.tight_layout()
        plt.savefig('cluster_metrics.png', dpi=300, bbox_inches='tight')
        print("‚úì –ì—Ä–∞—Ñ—ñ–∫–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª 'cluster_metrics.png'")
        plt.show()
        print()

    def visualize_clusters(self):
        """–ö—Ä–æ–∫ 8: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –Ω–∞ UMAP embedding"""
        print("=" * 60)
        print("–ö–†–û–ö 8: –í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (UMAP)")
        print("=" * 60)

        # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –ø–µ—Ä—à—ñ 2 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∏ UMAP –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        # –Ø–∫—â–æ —î 5 –∫–æ–º–ø–æ–Ω–µ–Ω—Ç, —Å—Ç–≤–æ—Ä—é—î–º–æ –æ–∫—Ä–µ–º–∏–π 2D UMAP –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó
        if self.umap_embedding.shape[1] > 2:
            print("–°—Ç–≤–æ—Ä–µ–Ω–Ω—è 2D UMAP –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó...")
            umap_2d = umap.UMAP(
                n_neighbors=15,
                min_dist=0.1,
                n_components=2,
                metric='euclidean',
                random_state=42
            )
            embedding_2d = umap_2d.fit_transform(self.df_normalized)
        else:
            embedding_2d = self.umap_embedding

        print(f"‚úì –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è 2D UMAP –ø—Ä–æ–µ–∫—Ü—ñ—è –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó\n")

        # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≥—Ä–∞—Ñ—ñ–∫—É
        plt.figure(figsize=(14, 10))

        clusters = self.df_features['cluster'].unique()
        colors = plt.cm.rainbow(np.linspace(0, 1, len(clusters)))

        for cluster, color in zip(sorted(clusters), colors):
            cluster_data = embedding_2d[self.df_features['cluster'] == cluster]
            plt.scatter(cluster_data[:, 0], cluster_data[:, 1],
                        c=[color], label=f'–ö–ª–∞—Å—Ç–µ—Ä {cluster}',
                        alpha=0.6, s=50, edgecolors='black', linewidth=0.5)

        # –î–æ–¥–∞–≤–∞–Ω–Ω—è —Ü–µ–Ω—Ç—Ä–æ—ó–¥—ñ–≤ (–≤ UMAP –ø—Ä–æ—Å—Ç–æ—Ä—ñ)
        for cluster in sorted(clusters):
            cluster_mask = self.df_features['cluster'] == cluster
            centroid_2d = embedding_2d[cluster_mask].mean(axis=0)
            plt.scatter(centroid_2d[0], centroid_2d[1],
                        c='black', marker='X', s=300,
                        edgecolors='yellow', linewidth=2, zorder=5)

        plt.xlabel('UMAP Component 1', fontsize=12)
        plt.ylabel('UMAP Component 2', fontsize=12)
        plt.title('–í—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤ (UMAP)', fontsize=14, fontweight='bold')
        plt.legend(loc='best', ncol=2)
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('clusters_umap.png', dpi=300, bbox_inches='tight')
        print("‚úì –ì—Ä–∞—Ñ—ñ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª 'clusters_umap.png'")
        plt.show()
        print()

    def cluster_statistics(self, centroids_df):
        """–ö—Ä–æ–∫ 9: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö"""
        print("=" * 60)
        print("–ö–†–û–ö 9: –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö")
        print("=" * 60)

        total_users = len(self.df_features)

        print(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤: {total_users}\n")

        for cluster in sorted(self.df_features['cluster'].unique()):
            cluster_size = len(self.df_features[self.df_features['cluster'] == cluster])
            cluster_percentage = (cluster_size / total_users) * 100

            print(f"{'‚îÄ' * 60}")
            print(f"–ö–õ–ê–°–¢–ï–† {cluster}")
            print(f"{'‚îÄ' * 60}")
            print(f"–ö—ñ–ª—å–∫—ñ—Å—Ç—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤: {cluster_size}")
            print(f"–í—ñ–¥—Å–æ—Ç–æ–∫ –≤—ñ–¥ –∑–∞–≥–∞–ª—å–Ω–æ—ó –∫—ñ–ª—å–∫–æ—Å—Ç—ñ: {cluster_percentage:.2f}%")
            print(f"\n–¶–µ–Ω—Ç—Ä–æ—ó–¥ –∫–ª–∞—Å—Ç–µ—Ä–∞ {cluster}:")
            print(centroids_df.iloc[cluster].round(2))
            print()

        # –ó–∞–≥–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –æ–∑–Ω–∞–∫–∞—Ö –≤ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö
        print(f"{'=' * 60}")
        print("–ü–û–†–Ü–í–ù–Ø–õ–¨–ù–ê –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–õ–ê–°–¢–ï–†–ê–•")
        print(f"{'=' * 60}")

        cluster_stats = self.df_features.groupby('cluster').mean()
        print("\n–°–µ—Ä–µ–¥–Ω—ñ –∑–Ω–∞—á–µ–Ω–Ω—è –æ–∑–Ω–∞–∫ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞—Ö:")
        print(cluster_stats.round(2))
        print()

        return cluster_stats


# –û—Å–Ω–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∑–∞–ø—É—Å–∫—É –≤—Å—å–æ–≥–æ –ø—Ä–æ—Ü–µ—Å—É
def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤"""

    # –®–ª—è—Ö –¥–æ —Ñ–∞–π–ª—É (–∑–º—ñ–Ω—ñ—Ç—å –Ω–∞ —Å–≤—ñ–π)
    filepath = 'user_features.csv'  # –ó–∞–º—ñ–Ω—ñ—Ç—å –Ω–∞ —à–ª—è—Ö –¥–æ –≤–∞—à–æ–≥–æ —Ñ–∞–π–ª—É

    # –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –æ–±'—î–∫—Ç–∞ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞—Ü—ñ—ó
    segmentation = UserSegmentation(filepath)

    # –í–∏–∫–æ–Ω–∞–Ω–Ω—è –≤—Å—ñ—Ö –∫—Ä–æ–∫—ñ–≤
    segmentation.load_and_clean_data()
    segmentation.df_cleaned.to_csv('cleaned.csv')
    segmentation.feature_selection()
    segmentation.normalize_data()

    K_range, inertias, silhouette_scores, db_scores, ch_scores, n_hdbscan = segmentation.find_optimal_clusters(max_k=20)

    # –ó–∞–ø–∏—Ç –º–µ—Ç–æ–¥—É —Ç–∞ –∫—ñ–ª—å–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –≤—ñ–¥ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á–∞
    print("=" * 60)
    print(f"HDBSCAN –∑–Ω–∞–π—à–æ–≤ {n_hdbscan} –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ")
    print(f"K-Means —Ä–µ–∫–æ–º–µ–Ω–¥—É—î {segmentation.optimal_k} –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (–∑–∞ Silhouette Score)")
    print()
    method_input = input("–í–∏–±–µ—Ä—ñ—Ç—å –º–µ—Ç–æ–¥ (1=HDBSCAN, 2=K-Means, Enter=K-Means): ").strip()

    if method_input == '1':
        method = 'hdbscan'
        print("\n‚úì –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è HDBSCAN")
        segmentation.create_clusters(method='hdbscan')
    else:
        method = 'kmeans'
        user_input = input(f"–í–≤–µ–¥—ñ—Ç—å –∫—ñ–ª—å–∫—ñ—Å—Ç—å –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤ (Enter –¥–ª—è {segmentation.optimal_k}): ")
        n_clusters = int(user_input) if user_input.strip() else None
        print(f"\n‚úì –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è K-Means –∑ {n_clusters if n_clusters else segmentation.optimal_k} –∫–ª–∞—Å—Ç–µ—Ä–∞–º–∏")
        segmentation.create_clusters(n_clusters, method='kmeans')

    print("=" * 60)
    print()

    centroids_df = segmentation.get_centroids()
    segmentation.plot_metrics(K_range, inertias, silhouette_scores, db_scores, ch_scores)
    segmentation.visualize_clusters()
    segmentation.cluster_statistics(centroids_df)

    print("=" * 60)
    print("–°–ï–ì–ú–ï–ù–¢–ê–¶–Ü–Ø –ó–ê–í–ï–†–®–ï–ù–ê –£–°–ü–Ü–®–ù–û!")
    print("=" * 60)
    print("\n–§–∞–π–ª–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ:")
    print("  ‚Ä¢ cluster_metrics.png - –≥—Ä–∞—Ñ—ñ–∫–∏ –º–µ—Ç—Ä–∏–∫ (4 –º–µ—Ç—Ä–∏–∫–∏)")
    print("  ‚Ä¢ clusters_umap.png - UMAP –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤")
    print("  ‚Ä¢ cluster_centroids.csv - —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∏ —É CSV —Ñ–æ—Ä–º–∞—Ç—ñ")
    print("  ‚Ä¢ cluster_centroids.txt - —Ü–µ–Ω—Ç—Ä–æ—ó–¥–∏ —É —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É —Ñ–æ—Ä–º–∞—Ç—ñ")
    print("\n–ú–µ—Ç–æ–¥–∏ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω—ñ:")
    print("  ‚Ä¢ UMAP –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä–Ω–æ—Å—Ç—ñ")
    print("  ‚Ä¢ HDBSCAN –¥–ª—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–Ω–æ–≥–æ –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∫–ª–∞—Å—Ç–µ—Ä—ñ–≤")
    print("  ‚Ä¢ K-Means –Ω–∞ UMAP embedding –¥–ª—è —Å—Ç–∞–±—ñ–ª—å–Ω–∏—Ö —Å–µ–≥–º–µ–Ω—Ç—ñ–≤")
    print("  ‚Ä¢ MinMaxScaler –¥–ª—è –Ω–æ—Ä–º–∞–ª—ñ–∑–∞—Ü—ñ—ó")
    print("  ‚Ä¢ 4 –º–µ—Ç—Ä–∏–∫–∏ —è–∫–æ—Å—Ç—ñ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü—ñ—ó")


if __name__ == "__main__":
    main()